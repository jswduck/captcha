{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6314c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed90c0",
   "metadata": {},
   "source": [
    "Build a Captcha Solver \n",
    "=================\n",
    "\n",
    "A website uses Captchas on a form to keep the web-bots away. However, the captchas it generates, are quite similar each time:\n",
    "- the number of characters remains the same each time  \n",
    "- the font and spacing is the same each time  \n",
    "- the background and foreground colors and texture, remain largely the same\n",
    "- there is no skew in the structure of the characters.  \n",
    "- the captcha generator, creates strictly 5-character captchas, and each of the characters is either an upper-case character (A-Z) or a numeral (0-9).\n",
    "\n",
    "\n",
    "A sample of 26 captcha has been provided. We are now to build a captch solver to identify the unseen captcha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f1bfc",
   "metadata": {},
   "source": [
    "## Solution based on tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89dbf6",
   "metadata": {},
   "source": [
    "Before we can use tesseract, let install tesseract:\n",
    "\n",
    "```shell\n",
    "brew install tesseract\n",
    "\n",
    "brew install tesseract-lang\n",
    "```\n",
    "\n",
    "And in the function <em>inference_with_tesseract</em> in the **Captcha** class, restrict tesseract to only look for uppercase letters and digits. Also set --psm 7 so that tesseract treats individual captcha as a single line\n",
    "\n",
    "```python\n",
    "    tesseract_results = [pytesseract.image_to_string(Image.open(path), config='--psm 7 --oem 1 -l eng -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789').strip()[:5] for path in batch_data['input_file']] # --oem 1 --tessdata-dir /opt/homebrew/share/tessdata\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f42922",
   "metadata": {
    "tag": [
     "tesseract"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesseract OCR for inference\n",
      "Generated text 'EGYK4' saved to ../data/output/pred00.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred01.txt\n",
      "Generated text '6OSW1' saved to ../data/output/pred02.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred03.txt\n",
      "Generated text 'VLI2C' saved to ../data/output/pred04.txt\n",
      "Generated text 'O1R70' saved to ../data/output/pred05.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred06.txt\n",
      "Generated text 'ZRMQU' saved to ../data/output/pred07.txt\n",
      "Generated text 'NDES' saved to ../data/output/pred08.txt\n",
      "Generated text '26553' saved to ../data/output/pred09.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred10.txt\n",
      "Generated text 'YMB10' saved to ../data/output/pred100.txt\n",
      "Generated text 'J14D' saved to ../data/output/pred11.txt\n",
      "Generated text 'POSAE' saved to ../data/output/pred12.txt\n",
      "Generated text 'ZD0' saved to ../data/output/pred13.txt\n",
      "Generated text 'WGST7' saved to ../data/output/pred14.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred15.txt\n",
      "Generated text '4AD2K' saved to ../data/output/pred16.txt\n",
      "Generated text '20BHQ' saved to ../data/output/pred17.txt\n",
      "Generated text 'OAHOV' saved to ../data/output/pred18.txt\n",
      "Generated text 'SI8VE' saved to ../data/output/pred19.txt\n",
      "Generated text '297ME' saved to ../data/output/pred20.txt\n",
      "Generated text 'L' saved to ../data/output/pred21.txt\n",
      "Generated text 'HCE91' saved to ../data/output/pred22.txt\n",
      "Generated text 'WELXY' saved to ../data/output/pred23.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred24.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from captcha import Captcha\n",
    "import util\n",
    "\n",
    "captcha_solver = Captcha(device, \"../config.yaml\")\n",
    "preds_tesseract = captcha_solver(mode = \"tesseract\", im_path = \"\") # folder with a batch of files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274417b8",
   "metadata": {},
   "source": [
    "### Evaluation of tesseract performance\n",
    "\n",
    "Each sample captcha has a groud truth label already. We will load them and merge with the results and calculate error at both the character level and word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a51e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   |\n",
      "+====+=======+============================+============================+==============+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 6OSW1        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VLI2C        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | O1R70        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMQU        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | NDES         |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | 26553        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB10        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14D         |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | POSAE        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | ZD0          |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGST7        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 4AD2K        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | OAHOV        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | SI8VE        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | 297ME        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | L            |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HCE91        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXY        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# let us look at the predicted text first\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['key', 'Image File', 'Output File', 'Prediction']\n",
    "print(tabulate(preds_tesseract, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76fe3e",
   "metadata": {},
   "source": [
    "And now load the existing labels. They are in output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0306872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   | GT Label   | Correct   |\n",
      "+====+=======+============================+============================+==============+============+===========+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        | EGYK4      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        | GRC35      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 6OSW1        | 6O5W1      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        | J627C      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VLI2C        | VLI2C      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | O1R70        | O1R7Q      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        | OYTAD      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMQU        | ZRMQU      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | NDES         | N9DQS      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | 26553        | ZGJS3      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        | GZMBA      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB10        | YMB1Q      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14D         | J14DM      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | POSAE        | PQ9AE      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | ZD0          | VWZDO      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGST7        | WGST7      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        | XKMS2      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 4AD2K        | 1D2KB      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        | 20BHQ      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | OAHOV        | OAH0V      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | SI8VE        | 5I8VE      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | 297ME        | Z97ME      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | L            | CL69V      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HCE91        | HCE91      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXY        | WELXV      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        | UHVFO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "preds_tesseract_with_labels = captcha_solver.add_gt_labels(preds_tesseract)\n",
    "\n",
    "headers = ['key','Image File', 'Output File', 'Prediction'  , 'GT Label', 'Correct']\n",
    "print(tabulate(preds_tesseract_with_labels, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15966a",
   "metadata": {},
   "source": [
    "Let's now analyze the error at both character level and word level, i.e., <em>CER</em> and <em>WER</em>.\n",
    "\n",
    "Load the library from HF first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf90c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the metric\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196e80c",
   "metadata": {},
   "source": [
    "#### CER and WER calculation\n",
    "\n",
    "Now calculate the CER and WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4c088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Character Error Rate (CER): 20.00%\n",
      "Overall Word Error Rate (WER): 53.85%\n",
      "\n",
      "Detailed Analysis:\n",
      "============================================================\n",
      "\n",
      "Sample 02:\n",
      "Expected: 6O5W1\n",
      "Predicted: 6OSW1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 2: Expected '5', Got 'S'\n",
      "Image file: ../data/input/input02.jpg\n",
      "\n",
      "Sample 05:\n",
      "Expected: O1R7Q\n",
      "Predicted: O1R70\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 4: Expected 'Q', Got '0'\n",
      "Image file: ../data/input/input05.jpg\n",
      "\n",
      "Sample 08:\n",
      "Expected: N9DQS\n",
      "Predicted: NDES\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected '9', Got 'D'\n",
      "Position 2: Expected 'D', Got 'E'\n",
      "Position 3: Expected 'Q', Got 'S'\n",
      "Length mismatch: Expected 5, Got 4\n",
      "Image file: ../data/input/input08.jpg\n",
      "\n",
      "Sample 09:\n",
      "Expected: ZGJS3\n",
      "Predicted: 26553\n",
      "CER: 80.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Position 1: Expected 'G', Got '6'\n",
      "Position 2: Expected 'J', Got '5'\n",
      "Position 3: Expected 'S', Got '5'\n",
      "Image file: ../data/input/input09.jpg\n",
      "\n",
      "Sample 100:\n",
      "Expected: YMB1Q\n",
      "Predicted: YMB10\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 4: Expected 'Q', Got '0'\n",
      "Image file: ../data/input/input100.jpg\n",
      "\n",
      "Sample 11:\n",
      "Expected: J14DM\n",
      "Predicted: J14D\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Length mismatch: Expected 5, Got 4\n",
      "Image file: ../data/input/input11.jpg\n",
      "\n",
      "Sample 12:\n",
      "Expected: PQ9AE\n",
      "Predicted: POSAE\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'Q', Got 'O'\n",
      "Position 2: Expected '9', Got 'S'\n",
      "Image file: ../data/input/input12.jpg\n",
      "\n",
      "Sample 13:\n",
      "Expected: VWZDO\n",
      "Predicted: ZD0\n",
      "CER: 60.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'V', Got 'Z'\n",
      "Position 1: Expected 'W', Got 'D'\n",
      "Position 2: Expected 'Z', Got '0'\n",
      "Length mismatch: Expected 5, Got 3\n",
      "Image file: ../data/input/input13.jpg\n",
      "\n",
      "Sample 16:\n",
      "Expected: 1D2KB\n",
      "Predicted: 4AD2K\n",
      "CER: 60.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected '1', Got '4'\n",
      "Position 1: Expected 'D', Got 'A'\n",
      "Position 2: Expected '2', Got 'D'\n",
      "Position 3: Expected 'K', Got '2'\n",
      "Position 4: Expected 'B', Got 'K'\n",
      "Image file: ../data/input/input16.jpg\n",
      "\n",
      "Sample 18:\n",
      "Expected: OAH0V\n",
      "Predicted: OAHOV\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/input/input18.jpg\n",
      "\n",
      "Sample 19:\n",
      "Expected: 5I8VE\n",
      "Predicted: SI8VE\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected '5', Got 'S'\n",
      "Image file: ../data/input/input19.jpg\n",
      "\n",
      "Sample 20:\n",
      "Expected: Z97ME\n",
      "Predicted: 297ME\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Image file: ../data/input/input20.jpg\n",
      "\n",
      "Sample 21:\n",
      "Expected: CL69V\n",
      "Predicted: L\n",
      "CER: 80.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'C', Got 'L'\n",
      "Length mismatch: Expected 5, Got 1\n",
      "Image file: ../data/input/input21.jpg\n",
      "\n",
      "Sample 23:\n",
      "Expected: WELXV\n",
      "Predicted: WELXY\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 4: Expected 'V', Got 'Y'\n",
      "Image file: ../data/input/input23.jpg\n"
     ]
    }
   ],
   "source": [
    "# Calculate CER & WER for all captcha samples\n",
    "predictions = preds_tesseract_with_labels['prediction'].tolist()\n",
    "references = preds_tesseract_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_tesseract_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42120143",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "It can be seen that tesseract (LSTM engine, with English) does not achieve very good performance on the sample set. It achieves a CER of 20%, and a WER of 53.85%. For some captcha, it identifies less than 5 characters. \n",
    "\n",
    "And with tesseract, preprocessing (cropping the text and remove the background) does not help in improving the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ec17a",
   "metadata": {},
   "source": [
    "## Solution based on TrOCR, without preprocessing\n",
    "\n",
    "We will use pre-trained TrOCR from Microsoft for this task. \n",
    "\n",
    "TrOCR has a family of pre-trained models, including **trocr-small-printed**, **trocr-base-printed**, **trocr-base-handwritten**, etc. Since we are dealing with clear and consistent texts, we will choose modle fine tuned with printed text, like **trocr-base-printed**. \n",
    "\n",
    "We also need to balance inference speed and accuracy. We will go with **trocr-base-printed** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb6d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import util\n",
    "\n",
    "captcha_solver = Captcha(device, \"../config.yaml\") # run again to reload the config.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea424518",
   "metadata": {},
   "source": [
    "Let's do it without pre-processing of the captha images first. This is a switch that can be set in the config.yaml. In the same config file, we can also change the pretrained model.\n",
    "\n",
    "```yaml\n",
    "model: \n",
    "    preprocessing: false\n",
    "    pretrained_path: \"microsoft/trocr-base-printed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c802f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TrOCR for inference\n",
      "Preprocessing turned off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsweng/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 'EGYK4' saved to ../data/output/pred00.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred01.txt\n",
      "Generated text '605W1' saved to ../data/output/pred02.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred03.txt\n",
      "Generated text 'VL12C' saved to ../data/output/pred04.txt\n",
      "Generated text '01R70' saved to ../data/output/pred05.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred06.txt\n",
      "Generated text 'ZRMOU' saved to ../data/output/pred07.txt\n",
      "Generated text 'N9DOS' saved to ../data/output/pred08.txt\n",
      "Generated text '2GJ53' saved to ../data/output/pred09.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred10.txt\n",
      "Generated text 'YMB1Q' saved to ../data/output/pred100.txt\n",
      "Generated text 'J14DM' saved to ../data/output/pred11.txt\n",
      "Generated text 'POSAE' saved to ../data/output/pred12.txt\n",
      "Generated text 'VWZDO' saved to ../data/output/pred13.txt\n",
      "Generated text 'WGSTZ' saved to ../data/output/pred14.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred15.txt\n",
      "Generated text '102KB' saved to ../data/output/pred16.txt\n",
      "Generated text '20BHQ' saved to ../data/output/pred17.txt\n",
      "Generated text 'DAHOV' saved to ../data/output/pred18.txt\n",
      "Generated text '518VE' saved to ../data/output/pred19.txt\n",
      "Generated text '29ZME' saved to ../data/output/pred20.txt\n",
      "Generated text 'CL69V' saved to ../data/output/pred21.txt\n",
      "Generated text 'HOE91' saved to ../data/output/pred22.txt\n",
      "Generated text 'WELXV' saved to ../data/output/pred23.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred24.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# captcha_solver(\"../data/input/input100.jpg\",\"../data/output/100.txt\") # individual file\n",
    "\n",
    "\n",
    "preds_trocr = captcha_solver(mode = \"TrOCR\", im_path = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9668e",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Like what we did with tesseract, we will evaluate performance with both CER and WER. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b516b99",
   "metadata": {},
   "source": [
    "Let us look at the predictions first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ace4bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   |\n",
      "+====+=======+============================+============================+==============+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 605W1        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VL12C        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | 01R70        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMOU        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | N9DOS        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | 2GJ53        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB1Q        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14DM        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | POSAE        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | VWZDO        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGSTZ        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 102KB        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | DAHOV        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | 518VE        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | 29ZME        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | CL69V        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HOE91        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXV        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# let us look at the predicted text first\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['key', 'Image File', 'Output File', 'Prediction']\n",
    "print(tabulate(preds_trocr, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27237ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   | GT Label   | Correct   |\n",
      "+====+=======+============================+============================+==============+============+===========+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        | EGYK4      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        | GRC35      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 605W1        | 6O5W1      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        | J627C      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VL12C        | VLI2C      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | 01R70        | O1R7Q      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        | OYTAD      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMOU        | ZRMQU      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | N9DOS        | N9DQS      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | 2GJ53        | ZGJS3      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        | GZMBA      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB1Q        | YMB1Q      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14DM        | J14DM      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | POSAE        | PQ9AE      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | VWZDO        | VWZDO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGSTZ        | WGST7      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        | XKMS2      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 102KB        | 1D2KB      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        | 20BHQ      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | DAHOV        | OAH0V      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | 518VE        | 5I8VE      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | 29ZME        | Z97ME      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | CL69V        | CL69V      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HOE91        | HCE91      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXV        | WELXV      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        | UHVFO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "preds_trocr_with_labels = captcha_solver.add_gt_labels(preds_trocr)\n",
    "\n",
    "headers = ['key','Image File', 'Output File', 'Prediction'  , 'GT Label', 'Correct']\n",
    "print(tabulate(preds_trocr_with_labels, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba27192",
   "metadata": {},
   "source": [
    "#### CER and WER calculation\n",
    "\n",
    "Let's calculate with the TrOCR inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc59dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Character Error Rate (CER): 13.85%\n",
      "Overall Word Error Rate (WER): 50.00%\n",
      "\n",
      "Detailed Analysis:\n",
      "============================================================\n",
      "\n",
      "Sample 02:\n",
      "Expected: 6O5W1\n",
      "Predicted: 605W1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'O', Got '0'\n",
      "Image file: ../data/input/input02.jpg\n",
      "\n",
      "Sample 04:\n",
      "Expected: VLI2C\n",
      "Predicted: VL12C\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 2: Expected 'I', Got '1'\n",
      "Image file: ../data/input/input04.jpg\n",
      "\n",
      "Sample 05:\n",
      "Expected: O1R7Q\n",
      "Predicted: 01R70\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got '0'\n",
      "Position 4: Expected 'Q', Got '0'\n",
      "Image file: ../data/input/input05.jpg\n",
      "\n",
      "Sample 07:\n",
      "Expected: ZRMQU\n",
      "Predicted: ZRMOU\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'Q', Got 'O'\n",
      "Image file: ../data/input/input07.jpg\n",
      "\n",
      "Sample 08:\n",
      "Expected: N9DQS\n",
      "Predicted: N9DOS\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'Q', Got 'O'\n",
      "Image file: ../data/input/input08.jpg\n",
      "\n",
      "Sample 09:\n",
      "Expected: ZGJS3\n",
      "Predicted: 2GJ53\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Position 3: Expected 'S', Got '5'\n",
      "Image file: ../data/input/input09.jpg\n",
      "\n",
      "Sample 12:\n",
      "Expected: PQ9AE\n",
      "Predicted: POSAE\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'Q', Got 'O'\n",
      "Position 2: Expected '9', Got 'S'\n",
      "Image file: ../data/input/input12.jpg\n",
      "\n",
      "Sample 14:\n",
      "Expected: WGST7\n",
      "Predicted: WGSTZ\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 4: Expected '7', Got 'Z'\n",
      "Image file: ../data/input/input14.jpg\n",
      "\n",
      "Sample 16:\n",
      "Expected: 1D2KB\n",
      "Predicted: 102KB\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'D', Got '0'\n",
      "Image file: ../data/input/input16.jpg\n",
      "\n",
      "Sample 18:\n",
      "Expected: OAH0V\n",
      "Predicted: DAHOV\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got 'D'\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/input/input18.jpg\n",
      "\n",
      "Sample 19:\n",
      "Expected: 5I8VE\n",
      "Predicted: 518VE\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'I', Got '1'\n",
      "Image file: ../data/input/input19.jpg\n",
      "\n",
      "Sample 20:\n",
      "Expected: Z97ME\n",
      "Predicted: 29ZME\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Position 2: Expected '7', Got 'Z'\n",
      "Image file: ../data/input/input20.jpg\n",
      "\n",
      "Sample 22:\n",
      "Expected: HCE91\n",
      "Predicted: HOE91\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'C', Got 'O'\n",
      "Image file: ../data/input/input22.jpg\n"
     ]
    }
   ],
   "source": [
    "# Calculate CER & WER for all captcha samples\n",
    "predictions = preds_trocr_with_labels['prediction'].tolist()\n",
    "references = preds_trocr_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_trocr_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152e622",
   "metadata": {},
   "source": [
    "CER is now 13.85%, and WER is 50%; while the same for tesseract are 20% and 53.85% respectively. \n",
    "\n",
    "There is an improvement, though not much. Let's see whether preprocessing can improve further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9fc0d",
   "metadata": {},
   "source": [
    "## Solution with TrOCR, with image pre-processing\n",
    "\n",
    "Let's see whether we can improve the performance with additional pre-processing steps on the captchas. \n",
    "\n",
    "Since the number of characters remains the same in each captcha, the font and spacing is the same each time, and the background and foreground colors and texture also remain largely the same, two preprocessing steps could help to enhance the images:\n",
    "\n",
    "- removing the background by setting threshold\n",
    "- cropping the image to bound the text only.\n",
    "\n",
    "**config.yaml** has a switch <em>preprocessing</em>. Setting it to true would turn on pre-processing. **REMEMBER** to turn it on before continue.\n",
    "\n",
    "```yaml\n",
    "model: \n",
    "    preprocessing: true\n",
    "    pretrained_path: \"microsoft/trocr-base-printed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bab85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TrOCR for inference\n",
      "Preprocessing turned on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsweng/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 'EGYK4' saved to ../data/output/pred00.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred01.txt\n",
      "Generated text '605W1' saved to ../data/output/pred02.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred03.txt\n",
      "Generated text 'VLI2C' saved to ../data/output/pred04.txt\n",
      "Generated text '01RZQ' saved to ../data/output/pred05.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred06.txt\n",
      "Generated text 'ZRMQU' saved to ../data/output/pred07.txt\n",
      "Generated text 'N9DQS' saved to ../data/output/pred08.txt\n",
      "Generated text 'ZGJ53' saved to ../data/output/pred09.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred10.txt\n",
      "Generated text 'YMB1Q' saved to ../data/output/pred100.txt\n",
      "Generated text 'J14DM' saved to ../data/output/pred11.txt\n",
      "Generated text 'PQ9AE' saved to ../data/output/pred12.txt\n",
      "Generated text 'VWZDO' saved to ../data/output/pred13.txt\n",
      "Generated text 'WGST7' saved to ../data/output/pred14.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred15.txt\n",
      "Generated text '1D2KB' saved to ../data/output/pred16.txt\n",
      "Generated text '20BHQ' saved to ../data/output/pred17.txt\n",
      "Generated text 'OAHOV' saved to ../data/output/pred18.txt\n",
      "Generated text '5I8VE' saved to ../data/output/pred19.txt\n",
      "Generated text 'Z97ME' saved to ../data/output/pred20.txt\n",
      "Generated text 'CL69V' saved to ../data/output/pred21.txt\n",
      "Generated text 'HCE91' saved to ../data/output/pred22.txt\n",
      "Generated text 'WELXV' saved to ../data/output/pred23.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred24.txt\n"
     ]
    }
   ],
   "source": [
    "captcha_solver = Captcha(device, \"../config.yaml\") # run again to reload the config.yaml\n",
    "\n",
    "preds_trocr_with_preprocessing = captcha_solver(mode = \"TrOCR\", im_path = \"\")  \n",
    "\n",
    "preds_trocr_with_preprocessing_with_labels = captcha_solver.add_gt_labels(preds_trocr_with_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645134c",
   "metadata": {},
   "source": [
    "Let's look at the effect of preprocessing before we further proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7c67b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAADaCAYAAAAhQDR7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKuJJREFUeJzt3Ql4VcX5x/EBAiE7kERBdhGhKiCIK1j3uuO+a933tY/aWuuuda1Wa7Vq3au11l1qcamiaEVRFIuCVVlURBIgCdkTsvyfd/7PSW+SeS93kgk3Id/P8/Aow+HcOeeeG857Z+Z3ejQ2NjYaAAAAAAioZ8idAQAAAICg0AAAAAAQHIUGAAAAgOAoNAAAAAAER6EBAAAAIDgKDQAAAADBUWgAAAAACI5CAwAAAEBwFBoAAAAAgqPQQBDXXHON6dGjR5v+7qOPPmr/7tKlS01HkX3La8hrAQAAoONRaHRzX3zxhTn++OPN4MGDTWpqqtlkk03McccdZ9u7o7ffftsWJM8++2yyuwIAANClUWh0Y88//7yZNGmSefPNN83JJ59s7r33XnPqqaeamTNn2vYXXngh4X1dccUVpqqqqk39OOGEE+zfHT58eJv+PgAAADqflGR3AMmxaNEie4O/6aabmlmzZpn8/PymP7vwwgvNzjvvbP/8P//5j91GU1FRYTIyMkxKSor91Ra9evWyvwAAALDhYESjm7rttttMZWWleeCBB5oVGSIvL8/cf//9toi49dZbW63DWLBggTn22GNN//79zdSpU5v9WSwZpbjgggvs/rKyssy0adPMDz/8YLeT7eOt0RgxYoQ54IADzHvvvWe2224707dvX1vwPP74481eo6ioyFxyySVm3LhxJjMz02RnZ5t9993XfPbZZ8HOVXRsX331lZ1mlpOTY8/ZlVdeaRobG833339vDjroIPvaAwcONLfffnuzv19bW2uuuuoqs80229i/K4WZFHIyctTS6tWrbYEn++rXr5858cQT7bG41pd8+eWX5vDDDzcDBgyw52fy5Mnm5ZdfDnbcAAAA7UGh0U1Nnz7d3szLDa/LT3/6U/vnr7zySqs/O+KII2yRcuONN5rTTz9dfY2TTjrJ3H333Wa//fYzt9xyi0lLSzP7779/wn385ptv7I30XnvtZW/epbCRfcauH1m8eLF58cUXbVFyxx13mEsvvdTMnz/f7LLLLmb58uUmpKOOOso0NDSYm2++2Wy//fbmhhtuMHfeeaftn6xxkWPcbLPNbOEjo0SR0tJS8+CDD5pdd93VbiOFy8qVK83ee+9t5s2b17Sd7PvAAw80Tz31lC0wfvvb35off/zR/n9Lcg522GEHs3DhQnPZZZfZ8yMFzMEHH+w15Q0AAKDDNKLbKSkpaZS3/qCDDoq73bRp0+x2paWl9vdXX321/f0xxxzTatvozyJz5861v7/ooouabXfSSSfZdtk+8sgjj9i2JUuWNLUNHz7cts2aNauprbCwsDE1NbXx4osvbmqrrq5urK+vb/Yash/Z7rrrrmvWJvuT14pn5syZdrtnnnmm1bGdccYZTW11dXWNQ4YMaezRo0fjzTff3NReXFzcmJaW1njiiSc227ampqbZ68h2G2+8ceMpp5zS1Pbcc8/Z17nzzjub2uTYdt9991Z932OPPRrHjRtnjz/S0NDQuNNOOzWOHj067jECAACsD4xodENlZWX2vzKdKZ7oz+Ub+VhnnXXWOl/j1Vdftf8955xzmrWff/75Cfdziy22aDbiItOVxowZY0cxIpKU1bPn/1/G9fX1duqRTKGS7T755BMT0mmnndb0/7KmRKYqydQpWUAfkelOLfso2/bp06dp1EKme9XV1dm/H9tHOWe9e/duNkokx3buuec264f8/bfeessceeSR9r1ctWqV/SXHLqMkX3/9tZ2iBgAAkEwsBu+GogIiKjh8C5KRI0eu8zW+/fZbe5PccluZWpSoYcOGtWqT6VPFxcVNv5cb97vuussmZi1ZssQWG5Hc3NyEX6st/ZH1FrI2QtagtGyXm/5Yjz32mJ3eJOsq1q5d29Qee37knA0aNMikp6fHPWcypUwKHFkjIr9cCgsL7XQuAACAZKHQ6IbkRlhuaCVRKh75c7lZlYXJsWStxfqgJVHJTXZE1onIzfYpp5xirr/+erswWgqciy66yBYhHd2fRPr4xBNP2LUlsn5C1pBstNFG9u/ddNNNNv3LV3RcshZERjBcfAo6AACAjkCh0U3J4uk///nPNtUpSo6K9e6779oUqDPPPLNN+5dnYsgNsYwyjB49utm38SHJg/V2220389BDDzVrLykpaTXSkCzSR0nMkueWxCZzXX311a3OmSRRyUL72FGNlucsihuWaVZ77rlnh/cfAACgLVij0U3JN+syMiGFRMtpPrIGQNZhyM2ubNcW0TftMqUplqRQhSQjA7GjB+KZZ57pVGsUolGP2H5++OGHZvbs2a3OmUyrkgIwIsXaPffc02w7GRGRBCuJIJZUqpYk0QoAACDZGNHopmSUQdYNHHfccfYZFLKgWdYLyCiGjA7I4mKJWR01alSb9i/PjDjssMNs/KsUMhLF+s4779hnUYiWz9xoz8jMddddZ59svtNOO9lo2yeffDLuQwbXN+mjjGYccsghNt5XRnnuu+8+u9i9vLy8aTuZWiXPDLn44ovtKMbYsWPtczGk8Gt5zqT4kJEoee9k8bgcb0FBgS1eli1bFvQ5IgAAAG1BodGNyfMw5GZW1gpExYUsoJapSJdffrnZaqut2rV/ebiePMBOChZ5toNM83n66adtKpMsog5B+ikPFvzrX/9q9z1p0iT77A95tkRnIeszVqxYYUcgXnvtNVtgyLoNGXl5++23m418SN/lyexSBMpaEylOZIrVlClTmp0z2cfHH39srr32WvsgPynmZKRj4sSJ9uGAAAAAydZDMm6T3Ql0H/KAOrkZlhttGU3BuskDCaXgkPU0UnAAAAB0BazRQIepqqpq1SZTqeSbennyONZ9ziSuV9a1SPKXjNYAAAB0FUydQoe59dZbzdy5c+1UrJSUFDNjxgz764wzzjBDhw5Ndvc6JXmgoRQbO+64o6mpqbFrO95//30b47u+YoUBAABCYOoUOswbb7xh1xAsWLDALnqWB96dcMIJ5je/+Y0tPNCarDWRB/vJYvDq6mr7PIyzzz7bnHfeecnuGgAAgBcKDQAAAADBsUYDAAAAQHAUGgAAAACCo9AAAAAAEFy7V+SuXbvWvWNlsW9dXV3C22tPj25oaHC2a9uHegq1dqy9e/du1VZbW+vctk+fPkGOaeXKlc52eWibi29/SkpKWrV9/vnnzm0nT57sdUzatSEPrPM576mpqQmfM1fUrtAeHKid9+nTpzvbBw0a5GyXhwn6HJPWH9f22vIq3/Mri8593r+cnByva1Kehp6oNWvWeL2mz2cyHu0caPtx/RyTKGKX9PR0Z3tZWZmzXUsXI0ABANDVMKIBAAAAIDgKDQAAAADBUWgAAAAACI5CAwAAAEBw7V5dqC0w1RbT+i7S9Nm31l5TU+O1mFhbYOqzWLlnz55e+9Zo50tb9K0tMNWOVdOvX792L5jVzpcv7f3TzqVrgbu2wNbXgAEDnO1bbLGF1wJebaG1z2JwjbZIXHuftL74+vTTTxPeVruWtEXf2jWgHavPtRHvWvVtd9EWiWt90c5NZmZmwq8JAEBnwIgGAAAAgOAoNAAAAAAER6EBAAAAIDgKDQAAAADBUWgAAAAASF7qVF1dnVe7lrajpcS4Upq0ZJfS0lKvxBpf2utqSVKu1B7ftCFt30VFRV7pR1q6lLZ/7f3zScPRzpf2XmvtvklMGlfikJZIpp2XyspKr/OlpVppr+srRFqbxjcdTHv/tOvDdc60875mzRpn+4cffuiVvlZSUuJs1z6XWjKUT9JcbW2tc9uMjAxn+9SpU736CABAV8OIBgAAAIDgKDQAAAAABEehAQAAACA4Cg0AAIAN0EknnWRGjBiR7G6gG6PQAAAACXv00Udt0EX0SwI7Nt98c3PeeeeZgoKCZHcPbbDrrruarbbaKtndwAYo4XiTDz74IEjqlE+SjbaPZcuWOduHDBnilSCkJTTV1NR4JdlkZmYm1CbKysq8UoW086v1PSsry9leWFjobM/Ozna2u1KBtH1rtMQl7fxqx6qdS01VVVXCqVBa+pF2fn3T1HzPgU+ymW8SlZaKpCUuaedA217bv+uYtPPYp08fZ7u2/fjx4xO+BuJdByHeP+36nTt3rvGh/ZzJzc312g/Q0a677jozcuRImzj33nvvmT/96U/mn//8p/n8889Nenp6srsHoBMgRxEAAHjbd999zeTJk+3/n3baabYYvuOOO8xLL71kjjnmGOffqaioUCOfQ1ufrwXAjalTAACg3XbffXf73yVLljStD5AR6UWLFpn99tvPjoofd9xxTaO0d955p9lyyy3t1KuNN97YnHnmmaa4uLjZPmV9wQEHHGBef/11s/XWW9ttt9hiC/P88887p3O988475pxzzrHP2Imd5XDvvffa15KR2k022cSce+65ztFDeWaP9LV///62SJFR07vuuqvZNl9++aU5/PDD7fOspD9SbL388sutnrVz7bXXmtGjR9ttpAiTZ+e88cYbTdusWLHCnHzyybaf0q9BgwaZgw46yCxdurTZvmbMmGF23nln2x85h/vvv7/54osvWvX9xRdftNOf5PXkvy+88IJpDzmfMh3umWeesedcRoR33HFHM3/+fPvn999/v9lss83s68nUq5b9fvfdd80RRxxhhg0bZo9v6NCh5he/+IVzxDl6jdi+u9aXJHrdoPNgRAMAALSbFBQtp/nJlMK9997b3mT/7ne/a5pSJTeHUhzIjfYFF1xgi5M//vGP5tNPPzX//ve/m00N/frrr81RRx1lzjrrLHPiiSeaRx55xN7Avvrqq2avvfZq1gcpMvLz881VV11lRzTENddcY2/699xzT3P22Web//73v3aa10cffdTstaQIkKJGbvgvvPBCM3DgQLNw4ULzj3/8w/5eyA3+lClTzODBg81ll11mb/7//ve/m4MPPtg899xz5pBDDml6zZtuusmO9Gy33Xb2QcMff/yx+eSTT5r6fNhhh9n9nX/++faGWqY4Sx++++67phvsv/zlL/aY5Rzecsstdiq49F3Op5yraDspxGR/crMur7t69eqmIqY9pFiQIkoKMyH7lnP0y1/+0hZvcr7lJv/WW281p5xyinnrrbeaFQ/SXznnck3MmTPH3H333Xb6u/xZ5JVXXrHv77hx4+z+ZX+nnnqqPcct+Vw36BwoNAAAgLc1a9aYVatW2TUacpMnazbkW2+5EY1dzyRFgdxARmQ9x4MPPmiefPJJc+yxxza177bbbmafffaxN6Gx7V999ZW9iT/00EPt7+UmdOzYseZXv/pVq0JDRhnefPNN06tXL/v7lStX2tf+2c9+ZkcGojVj8vfl2/onnnjC3rTK2jO5iZUiY968eaZfv37ONVxScMg39FKkROvY5GZbbvylP1GhITfPMjLywAMPOM+djKa8//775rbbbjOXXHJJU/uvf/3rpv8vLy+3N9NSrMTuRwqPMWPGmBtvvLGpXV5bvt2Xc5uTk2PbdtllF3vcw4cPN20lRZmM4EQFjYz0yHm64YYb7PsSrd2U8yfnWUY1om2lMIpdF3fGGWfYEZDLL7/cFlNyHqNjlqJCrqFoTeYee+xhR0li++573aBzYOoUAADwJiMEMnogU2KOPvpoe5MoU15afhMt32jHkhtCuRmWIkEKlejXNttsY/cxc+bMZtvLVKfoBj4KMfn5z39uv8WW6UexTj/99KYiQ/zrX/+yQRUXXXRRs2AK2U72IwWBkH3Jt+OyXWyRERsKUVRUZL+xP/LII22wS9RvGT2QEQcZefnhhx/strIPGa2QNhe5AZfwi7ffflud9iOjG1KQyHqX2PMkx7f99ts3nacff/zRFkdSgERFhpDzKyMc7SE3/LHTl+R1hYyexAbERO2LFy9udowRGV2Svu+00062cJPzLZYvX26nYsn7GRv8IkWSjHC057pBFxvRkIvDRUta0dJztBQXmc+YaNKMtrjrJz/5ibM91FBaNC+xpZYfhni0xB4t9UY7B9oPJvm2IdHzGy+dR+Y+JtpHTexc1Fix/wisK1kp3vst36Ileqxaupb8A+Eic2BD9F273rXPR0cO+2r71tq1Y9WuSe2YXO+HlmilJVdpaW3aa3b0AlAtvcolmr6R6M8CFq+iq7jnnntsrK38TJBv0+Vb9pafSfmzltN35OZbRkNkHUUiKYnyLXjLn6XyukK+QZcpThFJwYr17bff2v9K32LJTf6mm27a9OfRtK94Ea/ffPON/XfwyiuvtL+0vkuhJaM7st5C+in7lG/cTzjhhKakPPkZKN/4X3zxxfbc7bDDDnYkSG64o+OJipRo7Yv271p0DLIepCU5bpmu1VbRqEMkKmSkuHS1x96byKiFTGGTqVct71nk/Y/tu7zHLUlbbN99rxt0DkydAgAA3mTtQZQ6pZEb6pbFh3wxIzeLMgXGRUZJ1seXAL6iL5RkqpOMYLhEN8w//elPbfEiCVyyfkKm/Pz+97839913n50KJWT05MADD7SLuF977TVbvMj0Ixk1mThxYtPryTqN2GKqrY8RaAvtCyetPfpCUr5IkZEHGQWSaV0yVU2+RJERH1nkrX05F09HXjfoOBQaAABgvRk1apSd0iSLqhMpDKKRhNhRDVkfINb11Otojr+sNZARjNjRU5kqJdO/oj4JeQZI1NZS9PdlFFjbpuV6EVn/Ib9kvYUUH7JIPCo0oteVUQ35Jd/YS7LW7bffbteORH2Sm+t4rxcdo2ualhx3MsgMEHmPHnvsMTtKo810iPou73FLLdt8rxt0DqzRAAAA642scZBvvK+//nrndOyWsbMyjz82qlUSnB5//HF7U+76pj+W3KDLNKk//OEPzab/PvTQQ3YaTjRNdtKkSXbalUSntnz96O/JDb8sUJZYV1kX0ZIsPNem5coaAhntiB72KWlMLaf/yo20rHuItpFRE5keJYu+XVNQo9eTBexyLuSmPpqSFN3UL1iwwCRDNOIRe87l/1tGBcv6G5laJu+nFGMRiSluOV3d97pB58CIBgAAWG9koa8kF8k0IVnELMlIMkog38jLgl+5GZXnVERknYMkTUnSk6xnePjhh01BQYGNuV0XmU4jqUYSbyvrJKZNm2a/5Zdo1m233dYcf/zxdjuZ3iWxsTKVSW7aZRRCbuAlcUkWdcvUpmhdiiRMydpMWVAuoxzSl9mzZ9vY1s8++8xuJ4uwpSiRhcoysiHRts8++6xNuhLybb8stJabZ9lWpkFJMSX7koX1QooM6ZOs7ZBCSNrleGTtgyxil2/2JdpVyLmUokn6JjGzMmVJomTleROxN/Dri0yVksJJppnJdCk5FkkOc60vlUJK1rPI8ch5l23kuKQAie2773WDLlZoaIsutTmC2iJYrd21qFx7Te1Do+1bW2CqLYTWFmNqC41ccw21vmvzGkMtHtcWSLsWd4tZs2Y5212LrbQF6PLNjEts+kWsaDFcexfBaotsXUOq2vuhLVjXFs9r51e7xrRFz9q1qnF9PrR9rGvubCL7jrcf7Zr07Y+LfPPocy350q6ZEMeq9V377PnOfQY2JLJWQW7CZXRA4k7lXkKmQcmNv9xwxpJFznLTfOmll9oiQUYenn76aXWdREsyXUlu0OXmVR4YJzf+ErUqN7ixYRiyP0kukqJEpi/Jv+1ysywFRUSKAikaZBt5noOMXMi/l7KmQhY+RySWVhZBy/oMGaGQKUISCSvHEC2mljQpieKVNRhy/HJzLs/kkESniMS1yrf+N998s43ClX3JYnN5gJ/clEeieNcrrrjCFlbSbynEZI2IJFutb3Jep0+fbs+DFAbyc1CSw6TQmjBhQrNtpbh76qmn7PskzyaR91vOrYzQtHwwoc91g86hR6NvlFA703a09mioMJE0nA8++EBdmObTR99CQ4byXCTDOdEbW1/ajU7s8Gis6GFIid7syA85l+5eaMg3US6S8+1z7WmFhm/KWIhCQysotHbtmtGuSck4d3H98PddxKj9Q6m9T52p0NB+bsi3mQDik5tI+WZbHpqH7kVGlqRA1P6dRtfAGg0AAAAkhcwgaPmFl3y5JNPQfL9MQufDGg0AAAAkhazhkEX7Mv1JponJuhiZIiUL/c8666xkdw/tRKEBAACApJAHDcu6C3nWiCRpyTRqWdgu61Jyc3OT3T20E4UGAADolOTJ39iwyXpOWdyPDVOHFRraYleftB1tWy35xzfpSVtsrr2utr2rP9rDZLSF09pCaK3vkpoRgrZYWWL9WhozZoxzW20RrLaAV1t87HvNdOQ15nvNaAunQ4UCtDOzIW5ftHbfJ7f67Ef7DGvnXTLofdK+tMXm2jH5BhH40PoyZ84cr6AHecouAABdCYvBAQAAAARHoQEAAAAgOAoNAAAAAMFRaAAAAAAIjkIDAAAAQOdLnQqR/KNtr+1DS47RUm/kqZM+CUL19fXO9tLSUmf7/PnzE06d+u6777xSpIqKirySnrKzs53tWsqPdo6/+eabhM+7lqSlXRtffPGFs728vNzZnp6e7mwvKytztmdlZbVq23LLLb3ShrRrRjsHvteklrylJRS59qMlNGlJTNr1rl1LGu3zUVJS4mxfuHBhq7bCwkLntnl5ec72xYsXO9urqqq8jqm4uNgr1Urbv+scaO+d1pfx48d7JZgBANDVMKIBAAAAIDgKDQAAAADBUWgAAAAACI5CAwAAAEBwFBoAAAAAOl/qlJaqo6UcaWlMru21tJaNNtooSLqUlhRUUVHhbM/JyUk4XWn06NFeSTNaUpB2DrS+a0lPWkKRliDk2o+WqqO911oalZawpVmzZo3X+/HRRx8lnBimpW6lpqZ6nXctzSg/P9/rfaqpqUm4P9r1rn0mQ9E+H7m5uc72CRMmJNz32bNnO9sPO+wwr+vXJ70r3jU8c+ZMZ/u+++7rbAcAAP/DiAYAAACA4Cg0AAAAAARHoQEAAAAgOAoNAAAAAJ1vMTgAAF2RFgaxLlqYQ2ftb0f2uT196gjd5Tg78lx0xWPtiHPEeVjPhYaWtqO1a2lJmuzs7FZtBQUFzm0XL17sbN98882d7WVlZV7pR66+xEs/ysjISDhdSztfWipUeXm5sz0zM9P4WLlypVcqkktJSYmzvV+/fs72vn37Gh9a4pKWLlVYWOhsX7BgQau2UaNGeZ0XrV17P7Tz6Jtspp1LV2pWenq6V+KS9jnQkphc13W8z7bW7uq79hnT0te0z4dGOyaflDXfz3FHp30BANDV8C8jAAAAgOAoNAAAAAAER6EBAAAAIDgKDQAAAADBUWgAAAAASF7qVFVVlVcyjZYes2zZMmf78OHDW7VtvPHGXolL33//vbN96dKlXqlI2rFqyTfasfpYu3atV+pNqHQpLYnIdW60pKC6ujpnu5bCoyUuaSlK2vux0UYbJZw+pqUNjRgxwtmel5fnbM/NzQ1y3rV0KS3ZzHUutXQpX9pnWEsB0z43lZWVznbXdaNd774Rez/++KOzfciQIV6fYe1a1ZK0SJhCd9HVInXX1d+OijNNRuTxuvob78/j9bczRgQn4/wmK8a6PTpjJC//WgIAAAAIjkIDAAAAQHAUGgAAAACCo9AAAAAAEByFBgAAAIDgejS2c1l9aWmpsz0rK8trRXxJSUnCyT9aglB5ebmzvXfv3l6JMlqKlE9/tGQlLVUoJyfHK91GS/7Rzo1vMpbPZaGlQmkpUtp5115TSyjSuJKFtGQsn32IhoYGryQ07ZrU9q9dq67rQEt5euqpp5ztp556qrNd209nol2/2vnSfPfdd872oUOHOts/+eQTZ/uoUaMSThLDhpfQkqw0mmSk9nTGpKCO6lNnPNZ46C+6ws+0MPmYAACgS+pqN4DcdP4/zgO6AqZOAQAAAAiOQgMAAABAcBQaAAAAAIKj0AAAAADQ+RaDp6SkeKUfadunpaW1e6FTRkaG1/a+CVBlZWWmvbSEpoEDBzrbtXOgpeekpqZ6pYBpCVCuxYGrV6/2SuPS9r18+XJne25urrO9uLjY2a6l/LiSiLTFjtp1qh2r9ppaGpX2fmgpWLNnz3a2jx07NuHEJS2R7NBDD/V6n7REuezsbOPD9f5tsskmXu9HdXW1V6pX//79ne3Dhg0zPlauXOls33LLLb32AwBAd8SIBgAAAIDgKDQAAAAABMdzNAAAAIBOrkcXe+aNYEQDAAAAQHAUGgAAAACSN3WqtrbW2Z6enu71glo6T8+ePRNqi7ePtWvXOtu1dB4tAcuXtn+XMWPGeKXq+A51afvRaMlCroStAQMGePVRSxDS0qWKioqc7drraubNm5fwedeuMS1dSqPtRzsHmszMzISTobTrV0vp0pKYtDQ13yFa7f1btGhRq7aamhrnttoxffvtt17pYKtWrfJKzBoxYoTXzxQt2QsAAPwPIxoAAAAAgqPQAAAAABAchQYAAACA4Ii3BQCgE0RQJit+Eus+/+t6T9sTO9pRuJ66ph5dMMI2HkY0AAAAACRvRENLevKtqrRKzdWupTlp+9D6qCVm+aZauZJ/4u3fJ+VJU1lZ6ZX25ZuGo20/ffr0Vm2TJ0/22reWuPTaa6852/Pz853t8+fP90pXqqurS/g9Wr58ubN94MCBzvalS5c627VrVfscaOf966+/drb/8MMPJlFZWVleCUpTp041PrSUKu3zsd122yWcLrVkyRJn++DBg71Sp7RkM9/P2YIFC5ztrutJO34AALorRjQAAAAABEehAQAAACA4Cg0AAAAAwVFoAAAAAEjeYnBt4bRGW3jqs6C6qqrK6zVTU1O9Fo9r25eXlyfcR9GrV69WbTU1Nc5tfRe4a4u+NVrftdfVzsGgQYMSaou34Fl7zX322cf4SEtLc7Zr14drofGaNWuc206YMMHr+tWOSduPprq62tk+bdq0hBcra9eG9n7MmjXLq4+rVq1ytufl5Tnbtf643g/tOtWuR43v50O7ZrTP35AhQxL+zAMIp6PiYtsa/9kZY0PbE7nbGY+nO+nRjSKwGdEAAAAAEByFBgAAAIDgKDQAAAAABEehAQAAACA4Cg0AAAAAyUud+uyzz5ztRUVFXuk8+fn5zvbi4uJ2J83U1dUZH1pyzJw5c7z2U19fn3AygG8al+8xZWZmeiUc+KTn+KaAaceqHZPWF+1cNjQ0JHzt5ebmeiUrae+Hll7V0VwpVdpnTGsvKyvzes2srCyva0m79mbMmJFwupT2XmuvqR2rxpWAFW8/FRUVQVL4AADojvjXEgAAAEDyRjQAAADWh3jPC+ioZ2wAobTnGm3sgs/KiIcRDQAAAADBUWgAAAAACI5CAwAAAEDy1miMHDnS2b7ttts621evXu2VfDNu3LhEu2IqKyu9EmUGDhzobF+xYoWzfezYsV6pOq40pr59+xoftbW1Xsfkk07UljQf17FqqVOu1K14KVLaMdXU1DjbtXOpJf9kZGQknB7k2jZeMpbWrl1LvrRzPGDAgIT3ob0f/fv3d7aXlpY624cNG2Z8aAle7733XsKJZL7XOwAA6LwY0QAAAAAQHIUGAAAAgOCYpwAAgIeOilclthXoOoiwTQwjGgAAAACCo9AAAAAAkLypU1rqjZYA1a9fP2d7bm5uoi+ppgo1NDR06LDz0KFDne1aUo6W8uOSlpbmlTakKSkpcbanp6d77UdL+SkvL084dUt7n3xp6VJaKlJWVpaz3ZUwlZeX55V0lZqa6nWsWl98ksriJXW5Usm0vmjHpH1uBg8enPA10JYhX9fno3fv3kH2DQAAOi9GNAAAAAAER6EBAAAAIDgKDQAAAADBUWgAAAAACI7naAAAEFBbQw2S9RyNeP1dV5/i/XlXC3fguQiIxfWwngsNV+pNvPQYLT1n9erVzva6urp2v1FaatHrr7/u1ccpU6Z4va4rSWrVqlVex6+lRWkXunZutPeprKws4fOupShpKUe+H0at71ofs7OzvZK3fBLDqqurva7rgoICr5QqLblJ06dPn4TPTUZGhtfnYLfddvN6P1zpXW3h+nxo711OTk6Q1wQAAMnH1CkAAAAAwVFoAAAAAAiOQgMAAABAcBQaAAAAAIKj0AAAAACQvNQpLclm1qxZ7U4z0hKNtAQeLf3o4Ycf9kocqq+vd7a/+OKLzvajjz464f0sXLjQua12TK+88oqz/ZBDDvE6B7m5uc52LQVLS4z629/+1qptyZIlXqlF2rH6plRpx9rQ0JDw/rWUI+0a0F5z+PDhXvtJSUnx+nxoXJ8bn+OP10eNloSm7Udrd10H/fr1c25LJCC6go66Tte132TE37anT52xv8mI+U0Wfp6iM2BEAwAAAEBwFBoAAAAAgqPQAAAAABAchQYAAACA4Cg0AAAAACQvdaq4uNjZXlZW5mzfddddvRKgXO0VFRXObXv16mVCWLNmjbN90aJFzvaqqipn++OPP96qbeedd3ZuW1tb65UOoaUlzZkzx9m+9dZbO9tzcnK8Xvfjjz9u1TZy5EivPpaWlnolmGnbZ2dne/XdJ2lD27ampsbZXllZ6ZU4or3fvgklrtSwvLw8r3289NJLzvZhw4Y5259//nmvNLH8/Hxn+7JlyxK+TrV9AwCADbjQAABgQ9LV4j87Y387Y586Qnc5zo7U1c5hV+tvZ8XUKQAAAADBUWgAAAAACI5CAwAAAEBwFBoAAAAAkrcYvH///s729PR0r3aftB0t4UdLf8rNzXW2a+lVWhJTQUGBVxLRpEmTWrXV19d7pepUV1c729PS0pztWVlZXsekJWw1NDQ42zMyMlq1zZgxw7lt7969E95HvFSviRMneqVaaQu1Vq9enfC1ob1P2vsxdOhQr/Q17f3W3j+NK2FKS4LT9j18+HBn+6hRo5ztWjKUr+XLl7dqS01NdW7L4jsAADYcjGgAAAAACI5CAwAAAEBwFBoAAAAAgqPQAAAAABAchQYAAACA5KVOaYlLWgKUlmbUq1cvZ/vatWsTTlzSEq18EpREaWmpsz0/P9/ZXldXl3DfV61a5dxWSz/SUni09COt71pqj5YM5ZMOttlmm3klK2nnQNuPlgClpSL17ds34fdJuza0vmvnUUtN07bXPh8aLb3KlSSlJcFp12lhYaGzfcKECc527Zz17NnT6/1zJXVp169vGhcAAOi8GNEAAAAAEByFBgAAAIDgKDQAAAAABEehAQAAACA4Cg0AAAAAyUud0tJ5MjMz250upaUiaek5KSkpXolAWmJWdna2s33FihVe6TwFBQWt2iZOnOh1/FrftWQlbXvtnGnpUloqkuvcaGlDmry8PGf7p59+6mxfvny5V2qYdo25EqC0a0C7lrSUrsrKSq/ttb6Xl5d7va8VFRWt2gYNGuTctri42Ot6145Ju2Y02jl2pVHNmTPHa9/azx/t/dMSs7RkLO1z5vt5BQAA/8OIBgAAAIDgKDQAAAAABEehAQAAACA4Cg0AAAAAyVsM3pVpC3U12gJ3bT/a9okuVI63SNW33Ze2yNa18FtbYKsdk7bQfOjQoV6L7bVF6Nr+fRaOa8ev8QkziMc36KCsrKxVW1ZWlnNb7X3SFmunpaV57UejLR4fP358q7bU1FSvRda+i7u1a1Jr196Pr776yut1O5J2DQMA0FkxogEAAAAgOAoNAAAAAMFRaAAAAAAIjkIDAAAAQHAUGgAAAACC6xapU1o6kZa2o6XzaMk3tbW1CafYVFdXe6UNaWlGWruW0KTx2V47fq1dS+PS0nO0vmhJT9rrZmRkmERp77X2fmipRVoftf2UlpY62wcMGOCV6ORzHn2PSTs3mr59+yb8ur7XqXbNhEpi8r3GysvLW7VlZ2cH6QsAABsKRjQAAAAABEehAQAAACA4Cg0AAAAAwVFoAAAAAAiOQgMAAABAcN0idconhUj06dPH2a4l5aSmpiac8JOVleVs79GjR5CEH019fb1X4pCr/9q2WgJWUVGRs33evHlex1RSUuL1PrmSm7QUsJUrV3rtW2vX3m/XtSGKi4ud7VOnTvXaj0/Kmnb9ateelsrmy5XcpPVRu06149euPS0tSrsOtNfV9u/6HPt+hgEA2NAxogEAAAAgOAoNAAAAAMFRaAAAAAAIjkIDAAAAQHAUGgAAAACC6xapU760pBktdcmlvLzcax/aa9bW1gZJz9H06tUr4W21JB8tbSc7O9vZ3r9/f2f7xIkTjQ/tXLr66XOcIWnpSnPnznW2a/10HZN2/L5pUdp+fFOUtP34JGaF6ouWsKWdX+11165dm/DnOzMz03Qk7ZgAAOis+JcLAAAAQHAUGgAAAACCo9AAAAAAEByFBgAAAIDgKDQAAAAABNctUqcqKiq8Uly0pCdtP6Wlpa3ali5d6ty2srLSK9EpKyvL2V5WVua1Hy1txyeNSUu00hJ7ioqKnO3Lli1zto8fP77dfdTeDy35KD093dleWFjobM/JyXG2p6SkeKWJaalh2rG6rlWf1C1tH/GuSd/zrl0fGRkZ7e6jdqw1NTVeKVXadaD1R7uGFy5cmPB7GsqUKVM6dP8AAITGiAYAAACA4Cg0AAAAAARHoQEAAAAgOAoNAAAAAMFRaAAAAAAIrkejFucCAAAAAG3EiAYAAACA4Cg0AAAAAARHoQEAAAAgOAoNAAAAAMFRaAAAAAAIjkIDAAAAQHAUGgAAAACCo9AAAAAAEByFBgAAAAAT2v8Bg1GrkqyZhNAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WGST7\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "\n",
    "index = 15\n",
    "image_path = preds_trocr_with_preprocessing_with_labels['input_file'][index]  # Use the existing image for testing\n",
    "\n",
    "\n",
    "# Show original, and preprocessed image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Original image\n",
    "#original = Image.open(image_path)\n",
    "ax1.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Preprocessed image\n",
    "preprocessed = util.preprocess_captcha(image_path)\n",
    "ax2.imshow(preprocessed)\n",
    "ax2.set_title('Preprocessed Image')\n",
    "ax2.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the label\n",
    "print(f\"{preds_trocr_with_preprocessing_with_labels['ground_truth'][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260139fa",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The prediction from TrOCR after preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bfbae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   | GT Label   | Correct   |\n",
      "+====+=======+============================+============================+==============+============+===========+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        | EGYK4      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        | GRC35      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 605W1        | 6O5W1      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        | J627C      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VLI2C        | VLI2C      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | 01RZQ        | O1R7Q      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        | OYTAD      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMQU        | ZRMQU      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | N9DQS        | N9DQS      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | ZGJ53        | ZGJS3      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        | GZMBA      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB1Q        | YMB1Q      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14DM        | J14DM      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | PQ9AE        | PQ9AE      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | VWZDO        | VWZDO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGST7        | WGST7      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        | XKMS2      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 1D2KB        | 1D2KB      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        | 20BHQ      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | OAHOV        | OAH0V      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | 5I8VE        | 5I8VE      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | Z97ME        | Z97ME      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | CL69V        | CL69V      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HCE91        | HCE91      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXV        | WELXV      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        | UHVFO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "headers = ['key','Image File', 'Output File', 'Prediction'  , 'GT Label', 'Correct']\n",
    "print(tabulate(preds_trocr_with_preprocessing_with_labels, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a3b32",
   "metadata": {},
   "source": [
    "#### CER and WER calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f269f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Character Error Rate (CER): 3.85%\n",
      "Overall Word Error Rate (WER): 15.38%\n",
      "\n",
      "Detailed Analysis:\n",
      "============================================================\n",
      "\n",
      "Sample 02:\n",
      "Expected: 6O5W1\n",
      "Predicted: 605W1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'O', Got '0'\n",
      "Image file: ../data/input/input02.jpg\n",
      "\n",
      "Sample 05:\n",
      "Expected: O1R7Q\n",
      "Predicted: 01RZQ\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got '0'\n",
      "Position 3: Expected '7', Got 'Z'\n",
      "Image file: ../data/input/input05.jpg\n",
      "\n",
      "Sample 09:\n",
      "Expected: ZGJS3\n",
      "Predicted: ZGJ53\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'S', Got '5'\n",
      "Image file: ../data/input/input09.jpg\n",
      "\n",
      "Sample 18:\n",
      "Expected: OAH0V\n",
      "Predicted: OAHOV\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/input/input18.jpg\n"
     ]
    }
   ],
   "source": [
    "predictions = preds_trocr_with_preprocessing_with_labels['prediction'].tolist()\n",
    "references = preds_trocr_with_preprocessing_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_trocr_with_preprocessing_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0fdc2",
   "metadata": {},
   "source": [
    "**Good News!!**\n",
    "\n",
    "With preprocessing turned on, the performance on the 26 samples improved. CER improves from 13.85% to 3.85%, while WER improves from 50% to 15.38%. \n",
    "\n",
    "Let's see whether we can do more improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829a971",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The sample size of 26 is relatively small. We could try to augment the small set by manipulating the original images, e.g., adding noise, changing brightness, etc. We do not apply manipulations like rotation, twist, re-coloring, etc. because the unseen captchas are expected to be similar in terms of numbers of characters, font, spacing, background and foreground color and texture, and skewness. \n",
    "\n",
    "**augment_captcha** is the function defined in **util.py** to generate new images based on the existing 26 captcha. New images would be written to <em>data/augmented</em> folder, while the labels are written to <em>data/output</em> folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a3b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting images: 26it [00:00, 166.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size increased from 26 to 104 images\n",
      "Augmented images saved to: ../data/augmented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "augmented_data = util.augment_captcha(preds_trocr_with_preprocessing_with_labels, captcha_solver.aug_dir, captcha_solver.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f6743",
   "metadata": {},
   "source": [
    "Now apply the TrOCR model on those new images in the <em>data/augmented</em> folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b79d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TrOCR for inference\n",
      "Preprocessing turned on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsweng/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 'EGYK4' saved to ../data/output/pred101.txt\n",
      "Generated text 'EGYK4' saved to ../data/output/pred102.txt\n",
      "Generated text 'EGYK4' saved to ../data/output/pred103.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred104.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred105.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred106.txt\n",
      "Generated text '605W1' saved to ../data/output/pred107.txt\n",
      "Generated text '605W1' saved to ../data/output/pred108.txt\n",
      "Generated text '605W1' saved to ../data/output/pred109.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred110.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred111.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred112.txt\n",
      "Generated text 'VLI2C' saved to ../data/output/pred113.txt\n",
      "Generated text 'VL12C' saved to ../data/output/pred114.txt\n",
      "Generated text 'VLI2C' saved to ../data/output/pred115.txt\n",
      "Generated text '01R79' saved to ../data/output/pred116.txt\n",
      "Generated text '01RZQ' saved to ../data/output/pred117.txt\n",
      "Generated text '01RZQ' saved to ../data/output/pred118.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred119.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred120.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred121.txt\n",
      "Generated text 'ZRMQU' saved to ../data/output/pred122.txt\n",
      "Generated text '2RMOU' saved to ../data/output/pred123.txt\n",
      "Generated text 'ZRMQU' saved to ../data/output/pred124.txt\n",
      "Generated text 'N9DQS' saved to ../data/output/pred125.txt\n",
      "Generated text 'N9005' saved to ../data/output/pred126.txt\n",
      "Generated text 'N9DQS' saved to ../data/output/pred127.txt\n",
      "Generated text '2GJS3' saved to ../data/output/pred128.txt\n",
      "Generated text 'ZGJ53' saved to ../data/output/pred129.txt\n",
      "Generated text 'ZGJ53' saved to ../data/output/pred130.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred131.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred132.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred133.txt\n",
      "Generated text 'YMB1Q' saved to ../data/output/pred134.txt\n",
      "Generated text 'YMB1Q' saved to ../data/output/pred135.txt\n",
      "Generated text 'YMB1Q' saved to ../data/output/pred136.txt\n",
      "Generated text 'J14DM' saved to ../data/output/pred137.txt\n",
      "Generated text '214DM' saved to ../data/output/pred138.txt\n",
      "Generated text 'J14DM' saved to ../data/output/pred139.txt\n",
      "Generated text 'PQ9AE' saved to ../data/output/pred140.txt\n",
      "Generated text 'PQ9AE' saved to ../data/output/pred141.txt\n",
      "Generated text 'PQ9AE' saved to ../data/output/pred142.txt\n",
      "Generated text 'VWZDO' saved to ../data/output/pred143.txt\n",
      "Generated text 'WZ00' saved to ../data/output/pred144.txt\n",
      "Generated text 'VWZDO' saved to ../data/output/pred145.txt\n",
      "Generated text '#GST7' saved to ../data/output/pred146.txt\n",
      "Generated text 'WGST7' saved to ../data/output/pred147.txt\n",
      "Generated text 'WGST7' saved to ../data/output/pred148.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred149.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred150.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred151.txt\n",
      "Generated text '1D2KB' saved to ../data/output/pred152.txt\n",
      "Generated text '1D2KB' saved to ../data/output/pred153.txt\n",
      "Generated text '102KB' saved to ../data/output/pred154.txt\n",
      "Generated text '20BHQ' saved to ../data/output/pred155.txt\n",
      "Generated text '206H0' saved to ../data/output/pred156.txt\n",
      "Generated text '20BHQ' saved to ../data/output/pred157.txt\n",
      "Generated text 'DAHOV' saved to ../data/output/pred158.txt\n",
      "Generated text 'DAHOV' saved to ../data/output/pred159.txt\n",
      "Generated text 'OAHOV' saved to ../data/output/pred160.txt\n",
      "Generated text '5I8VE' saved to ../data/output/pred161.txt\n",
      "Generated text '5I8VE' saved to ../data/output/pred162.txt\n",
      "Generated text '5I8VE' saved to ../data/output/pred163.txt\n",
      "Generated text 'Z97ME' saved to ../data/output/pred164.txt\n",
      "Generated text 'Z97ME' saved to ../data/output/pred165.txt\n",
      "Generated text 'Z97ME' saved to ../data/output/pred166.txt\n",
      "Generated text 'CL69V' saved to ../data/output/pred167.txt\n",
      "Generated text 'CL69V' saved to ../data/output/pred168.txt\n",
      "Generated text 'CL69V' saved to ../data/output/pred169.txt\n",
      "Generated text 'HCE91' saved to ../data/output/pred170.txt\n",
      "Generated text 'HCE91' saved to ../data/output/pred171.txt\n",
      "Generated text 'HCE91' saved to ../data/output/pred172.txt\n",
      "Generated text 'WELXV' saved to ../data/output/pred173.txt\n",
      "Generated text 'WELXV' saved to ../data/output/pred174.txt\n",
      "Generated text 'WELXV' saved to ../data/output/pred175.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred176.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred177.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred178.txt\n"
     ]
    }
   ],
   "source": [
    "captcha_solver = Captcha(device, \"../config.yaml\") # run again to reload the config.yaml\n",
    "\n",
    "preds_trocr_augmented_with_preprocessing = captcha_solver(mode = \"TrOCR\", im_path = \"../data/augmented\")  # let's use the augmented data only\n",
    "\n",
    "preds_trocr_augmented_with_preprocessing_with_labels = captcha_solver.add_gt_labels(preds_trocr_augmented_with_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f7844",
   "metadata": {},
   "source": [
    "### CER and WER calculation\n",
    "\n",
    "Time to evalaute the CER and WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8482d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Character Error Rate (CER): 7.95%\n",
      "Overall Word Error Rate (WER): 25.64%\n",
      "\n",
      "Detailed Analysis:\n",
      "============================================================\n",
      "\n",
      "Sample 107:\n",
      "Expected: 6O5W1\n",
      "Predicted: 605W1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'O', Got '0'\n",
      "Image file: ../data/augmented/input107.jpg\n",
      "\n",
      "Sample 108:\n",
      "Expected: 6O5W1\n",
      "Predicted: 605W1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'O', Got '0'\n",
      "Image file: ../data/augmented/input108.jpg\n",
      "\n",
      "Sample 109:\n",
      "Expected: 6O5W1\n",
      "Predicted: 605W1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'O', Got '0'\n",
      "Image file: ../data/augmented/input109.jpg\n",
      "\n",
      "Sample 114:\n",
      "Expected: VLI2C\n",
      "Predicted: VL12C\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 2: Expected 'I', Got '1'\n",
      "Image file: ../data/augmented/input114.jpg\n",
      "\n",
      "Sample 116:\n",
      "Expected: O1R7Q\n",
      "Predicted: 01R79\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got '0'\n",
      "Position 4: Expected 'Q', Got '9'\n",
      "Image file: ../data/augmented/input116.jpg\n",
      "\n",
      "Sample 117:\n",
      "Expected: O1R7Q\n",
      "Predicted: 01RZQ\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got '0'\n",
      "Position 3: Expected '7', Got 'Z'\n",
      "Image file: ../data/augmented/input117.jpg\n",
      "\n",
      "Sample 118:\n",
      "Expected: O1R7Q\n",
      "Predicted: 01RZQ\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got '0'\n",
      "Position 3: Expected '7', Got 'Z'\n",
      "Image file: ../data/augmented/input118.jpg\n",
      "\n",
      "Sample 123:\n",
      "Expected: ZRMQU\n",
      "Predicted: 2RMOU\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Position 3: Expected 'Q', Got 'O'\n",
      "Image file: ../data/augmented/input123.jpg\n",
      "\n",
      "Sample 126:\n",
      "Expected: N9DQS\n",
      "Predicted: N9005\n",
      "CER: 60.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 2: Expected 'D', Got '0'\n",
      "Position 3: Expected 'Q', Got '0'\n",
      "Position 4: Expected 'S', Got '5'\n",
      "Image file: ../data/augmented/input126.jpg\n",
      "\n",
      "Sample 128:\n",
      "Expected: ZGJS3\n",
      "Predicted: 2GJS3\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Image file: ../data/augmented/input128.jpg\n",
      "\n",
      "Sample 129:\n",
      "Expected: ZGJS3\n",
      "Predicted: ZGJ53\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'S', Got '5'\n",
      "Image file: ../data/augmented/input129.jpg\n",
      "\n",
      "Sample 130:\n",
      "Expected: ZGJS3\n",
      "Predicted: ZGJ53\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'S', Got '5'\n",
      "Image file: ../data/augmented/input130.jpg\n",
      "\n",
      "Sample 138:\n",
      "Expected: J14DM\n",
      "Predicted: 214DM\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'J', Got '2'\n",
      "Image file: ../data/augmented/input138.jpg\n",
      "\n",
      "Sample 144:\n",
      "Expected: VWZDO\n",
      "Predicted: WZ00\n",
      "CER: 60.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'V', Got 'W'\n",
      "Position 1: Expected 'W', Got 'Z'\n",
      "Position 2: Expected 'Z', Got '0'\n",
      "Position 3: Expected 'D', Got '0'\n",
      "Length mismatch: Expected 5, Got 4\n",
      "Image file: ../data/augmented/input144.jpg\n",
      "\n",
      "Sample 146:\n",
      "Expected: WGST7\n",
      "Predicted: #GST7\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'W', Got '#'\n",
      "Image file: ../data/augmented/input146.jpg\n",
      "\n",
      "Sample 154:\n",
      "Expected: 1D2KB\n",
      "Predicted: 102KB\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'D', Got '0'\n",
      "Image file: ../data/augmented/input154.jpg\n",
      "\n",
      "Sample 156:\n",
      "Expected: 20BHQ\n",
      "Predicted: 206H0\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 2: Expected 'B', Got '6'\n",
      "Position 4: Expected 'Q', Got '0'\n",
      "Image file: ../data/augmented/input156.jpg\n",
      "\n",
      "Sample 158:\n",
      "Expected: OAH0V\n",
      "Predicted: DAHOV\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got 'D'\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/augmented/input158.jpg\n",
      "\n",
      "Sample 159:\n",
      "Expected: OAH0V\n",
      "Predicted: DAHOV\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got 'D'\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/augmented/input159.jpg\n",
      "\n",
      "Sample 160:\n",
      "Expected: OAH0V\n",
      "Predicted: OAHOV\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/augmented/input160.jpg\n"
     ]
    }
   ],
   "source": [
    "predictions = preds_trocr_augmented_with_preprocessing_with_labels['prediction'].tolist()\n",
    "references = preds_trocr_augmented_with_preprocessing_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_trocr_augmented_with_preprocessing_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2f105",
   "metadata": {},
   "source": [
    "On the augmented data, the performance on the 78 samples improved. CER drops to 7.95% from 3.85%, while WER drops to 25.64% from 15.38%. \n",
    "\n",
    "Let's do a character-level analysis to see which characters are recognized wronly more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d7eaa06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of most common confusions:\n",
      "\n",
      "Character '0' was mistaken for:\n",
      "  - 'O': 3 times\n",
      "\n",
      "Character '7' was mistaken for:\n",
      "  - 'Z': 2 times\n",
      "\n",
      "Character 'B' was mistaken for:\n",
      "  - '6': 1 times\n",
      "\n",
      "Character 'D' was mistaken for:\n",
      "  - '0': 3 times\n",
      "\n",
      "Character 'I' was mistaken for:\n",
      "  - '1': 1 times\n",
      "\n",
      "Character 'J' was mistaken for:\n",
      "  - '2': 1 times\n",
      "\n",
      "Character 'O' was mistaken for:\n",
      "  - '0': 6 times\n",
      "  - 'D': 2 times\n",
      "\n",
      "Character 'Q' was mistaken for:\n",
      "  - '0': 2 times\n",
      "  - '9': 1 times\n",
      "  - 'O': 1 times\n",
      "\n",
      "Character 'S' was mistaken for:\n",
      "  - '5': 3 times\n",
      "\n",
      "Character 'V' was mistaken for:\n",
      "  - 'W': 1 times\n",
      "\n",
      "Character 'W' was mistaken for:\n",
      "  - 'Z': 1 times\n",
      "  - '#': 1 times\n",
      "\n",
      "Character 'Z' was mistaken for:\n",
      "  - '2': 2 times\n",
      "  - '0': 1 times\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store confusion matrix data\n",
    "confusion_data = {}\n",
    "\n",
    "# Iterate through all predictions\n",
    "for _, row in preds_trocr_augmented_with_preprocessing_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    \n",
    "    # Compare characters\n",
    "    for expected, predicted in zip(ref, pred):\n",
    "        if expected != predicted:\n",
    "            # Initialize nested dictionary if needed\n",
    "            if expected not in confusion_data:\n",
    "                confusion_data[expected] = {}\n",
    "            if predicted not in confusion_data[expected]:\n",
    "                confusion_data[expected][predicted] = 0\n",
    "            \n",
    "            # Increment count\n",
    "            confusion_data[expected][predicted] += 1\n",
    "\n",
    "# Get all unique characters (both expected and predicted)\n",
    "all_chars = sorted(set(\n",
    "    list(confusion_data.keys()) + \n",
    "    [char for d in confusion_data.values() for char in d.keys()]\n",
    "))\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix_size = len(all_chars)\n",
    "confusion_matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "# Fill the confusion matrix\n",
    "for i, expected in enumerate(all_chars):\n",
    "    if expected in confusion_data:\n",
    "        for j, predicted in enumerate(all_chars):\n",
    "            if predicted in confusion_data[expected]:\n",
    "                confusion_matrix[i, j] = confusion_data[expected][predicted]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(confusion_matrix, \n",
    "            xticklabels=all_chars,\n",
    "            yticklabels=all_chars,\n",
    "            annot=True,  # Show numbers in cells\n",
    "            fmt='g',     # Format as integer\n",
    "            cmap='YlOrRd',  # Yellow to Orange to Red color scheme\n",
    "            square=True)    # Make cells square\n",
    "\n",
    "plt.title('Character Recognition Confusion Matrix')\n",
    "plt.xlabel('Predicted Character')\n",
    "plt.ylabel('Expected Character')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary of most common confusions:\")\n",
    "for expected in all_chars:\n",
    "    if expected in confusion_data and confusion_data[expected]:\n",
    "        print(f\"\\nCharacter '{expected}' was mistaken for:\")\n",
    "        sorted_mistakes = sorted(confusion_data[expected].items(), \n",
    "                               key=lambda x: x[1], \n",
    "                               reverse=True)\n",
    "        for predicted, count in sorted_mistakes:\n",
    "            print(f\"  - '{predicted}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f6a1",
   "metadata": {},
   "source": [
    "The heatmap above shows that digit \"0\" and alphabet \"O\" confused the model the most. This pair is known to be challenging for CV models, and visually difficult to diffentiate even for human eyes. Other difficult pairs here include \"S\" and \"5\", \"Z\" and \"2\". \n",
    "\n",
    "We could try to fine tune a base model with those augmented data. Due to time constraint, the fine-tuning was not run to complete. Below is the working code used for fine-tuning with torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2deba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 104\n",
      "Training samples: 83\n",
      "Test samples: 21\n",
      "\n",
      "Training set distribution:\n",
      "Original images: 21\n",
      "Augmented images: 62\n",
      "\n",
      "Test set distribution:\n",
      "Original images: 5\n",
      "Augmented images: 16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# split data into train and test\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    augmented_data,\n",
    "    test_size=0.2,  # 20% for test set\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(augmented_data)}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Print sample distribution\n",
    "def print_distribution(df, name):\n",
    "    original = len(df[~df['augmented']])\n",
    "    augmented = len(df[df['augmented']])\n",
    "    print(f\"\\n{name} set distribution:\")\n",
    "    print(f\"Original images: {original}\")\n",
    "    print(f\"Augmented images: {augmented}\")\n",
    "    \n",
    "print_distribution(train_df, \"Training\")\n",
    "print_distribution(test_df, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f380c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 83\n",
      "Test samples: 21\n",
      "\n",
      "Sample data:\n",
      "Input shape: torch.Size([3, 384, 384])\n",
      "Label shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_target_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame containing image paths and labels\n",
    "            processor: TrOCR processor for image and text processing\n",
    "            max_target_length: Maximum length for text encoding padding\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row from DataFrame\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(row['input_file']).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Encode the text label\n",
    "        labels = self.processor.tokenizer(\n",
    "            row['ground_truth'],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        \n",
    "        # Replace padding tokens with -100 for loss calculation\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        \n",
    "        # Return processed image and encoded labels\n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n",
    "        return encoding\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = CaptchaDataset(train_df, captcha_solver.processor)\n",
    "test_dataset = CaptchaDataset(test_df, captcha_solver.processor)\n",
    "\n",
    "# Verify the dataset\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Test a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nSample data:\")\n",
    "print(f\"Input shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "491a44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample batch:\n",
      "Pixel values shape: torch.Size([3, 384, 384])\n",
      "Labels shape: torch.Size([128])\n",
      "Original text: ZGJS3\n",
      "Encoded labels: [0, 1301, 534, 37337, 246, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "ZGJS3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test a single batch\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"\\nSample batch:\")\n",
    "print(f\"Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"Original text: {train_df['ground_truth'].iloc[0]}\")\n",
    "print(f\"Encoded labels: {sample['labels'].tolist()}\")\n",
    "labels = sample['labels']\n",
    "labels[labels == -100] = captcha_solver.processor.tokenizer.pad_token_id\n",
    "label_str = captcha_solver.processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "594e3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create dataloader\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5611d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# prep for finetuning\n",
    "'''\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\") # microsoft/trocr-base-stage1 has not been fine-tuned on any dataset. \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80538ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# make some configurations \n",
    "'''\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = captcha_solver.processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = captcha_solver.processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = captcha_solver.processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa91c25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1297eaff1e5944eaa5d7c52fd12a2dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 15.231753921508789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df32fde136b48d99c4867a23d1f9a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     17\u001b[0m   batch[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:617\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m    613\u001b[0m         labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:913\u001b[0m, in \u001b[0;36mTrOCRForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 913\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projection(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    930\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:673\u001b[0m, in \u001b[0;36mTrOCRDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    660\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    661\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    662\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    670\u001b[0m         use_cache,\n\u001b[1;32m    671\u001b[0m     )\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:398\u001b[0m, in \u001b[0;36mTrOCRDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    397\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 398\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    400\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "# fine tuning\n",
    "'''\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):  \n",
    "   # train\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for batch in tqdm(train_dataloader):\n",
    "      # get the inputs\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "   \n",
    "\n",
    "model.save_pretrained(\"../model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captcha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
