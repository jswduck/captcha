{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6314c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed90c0",
   "metadata": {},
   "source": [
    "Build a Captcha Solver \n",
    "=================\n",
    "\n",
    "A website uses Captchas on a form to keep the web-bots away. However, the captchas it generates, are quite similar each time:\n",
    "- the number of characters remains the same each time  \n",
    "- the font and spacing is the same each time  \n",
    "- the background and foreground colors and texture, remain largely the same\n",
    "- there is no skew in the structure of the characters.  \n",
    "- the captcha generator, creates strictly 5-character captchas, and each of the characters is either an upper-case character (A-Z) or a numeral (0-9).\n",
    "\n",
    "\n",
    "A sample of 26 captcha has been provided. We are now to build a captch solver to identify the unseen captcha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f1bfc",
   "metadata": {},
   "source": [
    "## Solution based on tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89dbf6",
   "metadata": {},
   "source": [
    "Before we can use tesseract, let install tesseract:\n",
    "\n",
    "```shell\n",
    "brew install tesseract\n",
    "\n",
    "brew install tesseract-lang\n",
    "```\n",
    "\n",
    "And in the function <em>inference_with_tesseract</em> in the **Captcha** class, restrict tesseract to only look for uppercase letters and digits. Also set --psm 7 so that tesseract treats individual captcha as a single line\n",
    "\n",
    "```python\n",
    "    tesseract_results = [pytesseract.image_to_string(Image.open(path), config='--psm 7 --oem 1 -l eng -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789').strip()[:5] for path in batch_data['input_file']] # --oem 1 --tessdata-dir /opt/homebrew/share/tessdata\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f42922",
   "metadata": {
    "tag": [
     "tesseract"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from captcha import Captcha\n",
    "import util\n",
    "\n",
    "captcha_solver = Captcha(device, \"../config.yaml\")\n",
    "preds_tesseract = captcha_solver(mode = \"tesseract\", im_path = \"\") # folder with a batch of files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274417b8",
   "metadata": {},
   "source": [
    "### Evaluation of tesseract performance\n",
    "\n",
    "Each sample captcha has a groud truth label already. We will load them and merge with the results and calculate error at both the character level and word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us look at the predicted text first\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['key', 'Image File', 'Output File', 'Prediction']\n",
    "print(tabulate(preds_tesseract, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76fe3e",
   "metadata": {},
   "source": [
    "And now load the existing labels. They are in output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0306872",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_tesseract_with_labels = captcha_solver.add_gt_labels(preds_tesseract)\n",
    "\n",
    "headers = ['key','Image File', 'Output File', 'Prediction'  , 'GT Label', 'Correct']\n",
    "print(tabulate(preds_tesseract_with_labels, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15966a",
   "metadata": {},
   "source": [
    "Let's now analyze the error at both character level and word level, i.e., <em>CER</em> and <em>WER</em>.\n",
    "\n",
    "Load the library from HF first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf90c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the metric\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196e80c",
   "metadata": {},
   "source": [
    "#### CER and WER calculation\n",
    "\n",
    "Now calculate the CER and WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CER & WER for all captcha samples\n",
    "predictions = preds_tesseract_with_labels['prediction'].tolist()\n",
    "references = preds_tesseract_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_tesseract_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42120143",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "It can be seen that tesseract (LSTM engine, with English) does not achieve very good performance on the sample set. It achieves a CER of 20%, and a WER of 53.85%. For some captcha, it identifies less than 5 characters. \n",
    "\n",
    "And with tesseract, preprocessing (cropping the text and remove the background) does not help in improving the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ec17a",
   "metadata": {},
   "source": [
    "## Solution based on TrOCR, without preprocessing\n",
    "\n",
    "We will use pre-trained TrOCR from Microsoft for this task. \n",
    "\n",
    "TrOCR has a family of pre-trained models, including **trocr-small-printed**, **trocr-base-printed**, **trocr-base-handwritten**, etc. Since we are dealing with clear and consistent texts, we will choose modle fine tuned with printed text, like **trocr-base-printed**. \n",
    "\n",
    "We also need to balance inference speed and accuracy. We will go with **trocr-base-printed** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb6d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from captcha import Captcha\n",
    "import util\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "captcha_solver = Captcha(device, \"../config.yaml\") # run again to reload the config.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea424518",
   "metadata": {},
   "source": [
    "Let's do it without pre-processing of the captha images first. This is a switch that can be set in the config.yaml. In the same config file, we can also change the pretrained model.\n",
    "\n",
    "```yaml\n",
    "model: \n",
    "    preprocessing: false\n",
    "    pretrained_path: \"microsoft/trocr-base-printed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c802f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TrOCR for inference\n",
      "Preprocessing turned off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsweng/.pyenv/versions/3.10.13/envs/captcha/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 'EGYK4' saved to ../data/output/pred00.txt\n",
      "Generated text 'GRC35' saved to ../data/output/pred01.txt\n",
      "Generated text '605W1' saved to ../data/output/pred02.txt\n",
      "Generated text 'J627C' saved to ../data/output/pred03.txt\n",
      "Generated text 'VL12C' saved to ../data/output/pred04.txt\n",
      "Generated text '01R70' saved to ../data/output/pred05.txt\n",
      "Generated text 'OYTAD' saved to ../data/output/pred06.txt\n",
      "Generated text 'ZRMOU' saved to ../data/output/pred07.txt\n",
      "Generated text 'N9DOS' saved to ../data/output/pred08.txt\n",
      "Generated text '2GJ53' saved to ../data/output/pred09.txt\n",
      "Generated text 'GZMBA' saved to ../data/output/pred10.txt\n",
      "Generated text 'YMB1Q' saved to ../data/output/pred100.txt\n",
      "Generated text 'J14DM' saved to ../data/output/pred11.txt\n",
      "Generated text 'POSAE' saved to ../data/output/pred12.txt\n",
      "Generated text 'VWZDO' saved to ../data/output/pred13.txt\n",
      "Generated text 'WGSTZ' saved to ../data/output/pred14.txt\n",
      "Generated text 'XKMS2' saved to ../data/output/pred15.txt\n",
      "Generated text '102KB' saved to ../data/output/pred16.txt\n",
      "Generated text '20BHQ' saved to ../data/output/pred17.txt\n",
      "Generated text 'DAHOV' saved to ../data/output/pred18.txt\n",
      "Generated text '518VE' saved to ../data/output/pred19.txt\n",
      "Generated text '29ZME' saved to ../data/output/pred20.txt\n",
      "Generated text 'CL69V' saved to ../data/output/pred21.txt\n",
      "Generated text 'HOE91' saved to ../data/output/pred22.txt\n",
      "Generated text 'WELXV' saved to ../data/output/pred23.txt\n",
      "Generated text 'UHVFO' saved to ../data/output/pred24.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# captcha_solver(\"../data/input/input100.jpg\",\"../data/output/100.txt\") # individual file\n",
    "\n",
    "\n",
    "preds_trocr = captcha_solver(im_path = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9668e",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Like what we did with tesseract, we will evaluate performance with both CER and WER. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b516b99",
   "metadata": {},
   "source": [
    "Let us look at the predictions first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1621b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the metric\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ace4bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   |\n",
      "+====+=======+============================+============================+==============+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 605W1        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VL12C        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | 01R70        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMOU        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | N9DOS        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | 2GJ53        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB1Q        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14DM        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | POSAE        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | VWZDO        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGSTZ        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 102KB        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | DAHOV        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | 518VE        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | 29ZME        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | CL69V        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HOE91        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXV        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        |\n",
      "+----+-------+----------------------------+----------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# let us look at the predicted text first\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['key', 'Image File', 'Output File', 'Prediction']\n",
    "print(tabulate(preds_trocr, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27237ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|    |   key | Image File                 | Output File                | Prediction   | GT Label   | Correct   |\n",
      "+====+=======+============================+============================+==============+============+===========+\n",
      "|  0 |    00 | ../data/input/input00.jpg  | ../data/output/pred00.txt  | EGYK4        | EGYK4      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  1 |    01 | ../data/input/input01.jpg  | ../data/output/pred01.txt  | GRC35        | GRC35      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  2 |    02 | ../data/input/input02.jpg  | ../data/output/pred02.txt  | 605W1        | 6O5W1      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  3 |    03 | ../data/input/input03.jpg  | ../data/output/pred03.txt  | J627C        | J627C      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  4 |    04 | ../data/input/input04.jpg  | ../data/output/pred04.txt  | VL12C        | VLI2C      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  5 |    05 | ../data/input/input05.jpg  | ../data/output/pred05.txt  | 01R70        | O1R7Q      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  6 |    06 | ../data/input/input06.jpg  | ../data/output/pred06.txt  | OYTAD        | OYTAD      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  7 |    07 | ../data/input/input07.jpg  | ../data/output/pred07.txt  | ZRMOU        | ZRMQU      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  8 |    08 | ../data/input/input08.jpg  | ../data/output/pred08.txt  | N9DOS        | N9DQS      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "|  9 |    09 | ../data/input/input09.jpg  | ../data/output/pred09.txt  | 2GJ53        | ZGJS3      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 10 |    10 | ../data/input/input10.jpg  | ../data/output/pred10.txt  | GZMBA        | GZMBA      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 11 |   100 | ../data/input/input100.jpg | ../data/output/pred100.txt | YMB1Q        | YMB1Q      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 12 |    11 | ../data/input/input11.jpg  | ../data/output/pred11.txt  | J14DM        | J14DM      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 13 |    12 | ../data/input/input12.jpg  | ../data/output/pred12.txt  | POSAE        | PQ9AE      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 14 |    13 | ../data/input/input13.jpg  | ../data/output/pred13.txt  | VWZDO        | VWZDO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 15 |    14 | ../data/input/input14.jpg  | ../data/output/pred14.txt  | WGSTZ        | WGST7      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 16 |    15 | ../data/input/input15.jpg  | ../data/output/pred15.txt  | XKMS2        | XKMS2      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 17 |    16 | ../data/input/input16.jpg  | ../data/output/pred16.txt  | 102KB        | 1D2KB      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 18 |    17 | ../data/input/input17.jpg  | ../data/output/pred17.txt  | 20BHQ        | 20BHQ      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 19 |    18 | ../data/input/input18.jpg  | ../data/output/pred18.txt  | DAHOV        | OAH0V      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 20 |    19 | ../data/input/input19.jpg  | ../data/output/pred19.txt  | 518VE        | 5I8VE      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 21 |    20 | ../data/input/input20.jpg  | ../data/output/pred20.txt  | 29ZME        | Z97ME      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 22 |    21 | ../data/input/input21.jpg  | ../data/output/pred21.txt  | CL69V        | CL69V      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 23 |    22 | ../data/input/input22.jpg  | ../data/output/pred22.txt  | HOE91        | HCE91      | False     |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 24 |    23 | ../data/input/input23.jpg  | ../data/output/pred23.txt  | WELXV        | WELXV      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n",
      "| 25 |    24 | ../data/input/input24.jpg  | ../data/output/pred24.txt  | UHVFO        | UHVFO      | True      |\n",
      "+----+-------+----------------------------+----------------------------+--------------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "preds_trocr_with_labels = captcha_solver.add_gt_labels(preds_trocr)\n",
    "\n",
    "headers = ['key','Image File', 'Output File', 'Prediction'  , 'GT Label', 'Correct']\n",
    "print(tabulate(preds_trocr_with_labels, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba27192",
   "metadata": {},
   "source": [
    "#### CER and WER calculation\n",
    "\n",
    "Let's calculate with the TrOCR inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc59dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Character Error Rate (CER): 13.85%\n",
      "Overall Word Error Rate (WER): 50.00%\n",
      "\n",
      "Detailed Analysis:\n",
      "============================================================\n",
      "\n",
      "Sample 02:\n",
      "Expected: 6O5W1\n",
      "Predicted: 605W1\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'O', Got '0'\n",
      "Image file: ../data/input/input02.jpg\n",
      "\n",
      "Sample 04:\n",
      "Expected: VLI2C\n",
      "Predicted: VL12C\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 2: Expected 'I', Got '1'\n",
      "Image file: ../data/input/input04.jpg\n",
      "\n",
      "Sample 05:\n",
      "Expected: O1R7Q\n",
      "Predicted: 01R70\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got '0'\n",
      "Position 4: Expected 'Q', Got '0'\n",
      "Image file: ../data/input/input05.jpg\n",
      "\n",
      "Sample 07:\n",
      "Expected: ZRMQU\n",
      "Predicted: ZRMOU\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'Q', Got 'O'\n",
      "Image file: ../data/input/input07.jpg\n",
      "\n",
      "Sample 08:\n",
      "Expected: N9DQS\n",
      "Predicted: N9DOS\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 3: Expected 'Q', Got 'O'\n",
      "Image file: ../data/input/input08.jpg\n",
      "\n",
      "Sample 09:\n",
      "Expected: ZGJS3\n",
      "Predicted: 2GJ53\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Position 3: Expected 'S', Got '5'\n",
      "Image file: ../data/input/input09.jpg\n",
      "\n",
      "Sample 12:\n",
      "Expected: PQ9AE\n",
      "Predicted: POSAE\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'Q', Got 'O'\n",
      "Position 2: Expected '9', Got 'S'\n",
      "Image file: ../data/input/input12.jpg\n",
      "\n",
      "Sample 14:\n",
      "Expected: WGST7\n",
      "Predicted: WGSTZ\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 4: Expected '7', Got 'Z'\n",
      "Image file: ../data/input/input14.jpg\n",
      "\n",
      "Sample 16:\n",
      "Expected: 1D2KB\n",
      "Predicted: 102KB\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'D', Got '0'\n",
      "Image file: ../data/input/input16.jpg\n",
      "\n",
      "Sample 18:\n",
      "Expected: OAH0V\n",
      "Predicted: DAHOV\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'O', Got 'D'\n",
      "Position 3: Expected '0', Got 'O'\n",
      "Image file: ../data/input/input18.jpg\n",
      "\n",
      "Sample 19:\n",
      "Expected: 5I8VE\n",
      "Predicted: 518VE\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'I', Got '1'\n",
      "Image file: ../data/input/input19.jpg\n",
      "\n",
      "Sample 20:\n",
      "Expected: Z97ME\n",
      "Predicted: 29ZME\n",
      "CER: 40.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 0: Expected 'Z', Got '2'\n",
      "Position 2: Expected '7', Got 'Z'\n",
      "Image file: ../data/input/input20.jpg\n",
      "\n",
      "Sample 22:\n",
      "Expected: HCE91\n",
      "Predicted: HOE91\n",
      "CER: 20.00%\n",
      "WER: 100.00%\n",
      "Character differences:\n",
      "Position 1: Expected 'C', Got 'O'\n",
      "Image file: ../data/input/input22.jpg\n"
     ]
    }
   ],
   "source": [
    "# Calculate CER & WER for all captcha samples\n",
    "predictions = preds_trocr_with_labels['prediction'].tolist()\n",
    "references = preds_trocr_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_trocr_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152e622",
   "metadata": {},
   "source": [
    "CER is now 13.85%, and WER is 50%; while the same for tesseract are 20% and 53.85% respectively. \n",
    "\n",
    "There is an improvement, though not much. Let's see whether preprocessing can improve further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9fc0d",
   "metadata": {},
   "source": [
    "## Solution with TrOCR, with image pre-processing\n",
    "\n",
    "Let's see whether we can improve the performance with additional pre-processing steps on the captchas. \n",
    "\n",
    "Since the number of characters remains the same in each captcha, the font and spacing is the same each time, and the background and foreground colors and texture also remain largely the same, two preprocessing steps could help to enhance the images:\n",
    "\n",
    "- removing the background by setting threshold\n",
    "- cropping the image to bound the text only.\n",
    "\n",
    "**config.yaml** has a switch <em>preprocessing</em>. Setting it to true would turn on pre-processing. **REMEMBER** to turn it on before continue.\n",
    "\n",
    "```yaml\n",
    "model: \n",
    "    preprocessing: true\n",
    "    pretrained_path: \"microsoft/trocr-base-printed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "captcha_solver = Captcha(device, \"../config.yaml\") # run again to reload the config.yaml\n",
    "\n",
    "preds_trocr_with_preprocessing = captcha_solver(im_path = \"\")  \n",
    "\n",
    "preds_trocr_with_preprocessing_with_labels = captcha_solver.add_gt_labels(preds_trocr_with_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645134c",
   "metadata": {},
   "source": [
    "Let's look at the effect of preprocessing before we further proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "index = 15\n",
    "image_path = preds_trocr_with_preprocessing_with_labels['input_file'][index]  # Use the existing image for testing\n",
    "\n",
    "\n",
    "# Show original, and preprocessed image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Original image\n",
    "#original = Image.open(image_path)\n",
    "ax1.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Preprocessed image\n",
    "preprocessed = util.preprocess_captcha(image_path)\n",
    "ax2.imshow(preprocessed)\n",
    "ax2.set_title('Preprocessed Image')\n",
    "ax2.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the label\n",
    "print(f\"{preds_trocr_with_preprocessing_with_labels['ground_truth'][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260139fa",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The prediction from TrOCR after preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['key','Image File', 'Output File', 'Prediction'  , 'GT Label', 'Correct']\n",
    "print(tabulate(preds_trocr_with_preprocessing_with_labels, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a3b32",
   "metadata": {},
   "source": [
    "#### CER and WER calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = preds_trocr_with_preprocessing_with_labels['prediction'].tolist()\n",
    "references = preds_trocr_with_preprocessing_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_trocr_with_preprocessing_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0fdc2",
   "metadata": {},
   "source": [
    "**Good News!!**\n",
    "\n",
    "With preprocessing turned on, the performance on the 26 samples improved. CER improves from 13.85% to 3.85%, while WER improves from 50% to 15.38%. \n",
    "\n",
    "Let's see whether we can do more improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829a971",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The sample size of 26 is relatively small. We could try to augment the small set by manipulating the original images, e.g., adding noise, changing brightness, etc. We do not apply manipulations like rotation, twist, re-coloring, etc. because the unseen captchas are expected to be similar in terms of numbers of characters, font, spacing, background and foreground color and texture, and skewness. \n",
    "\n",
    "**augment_captcha** is the function defined in **util.py** to generate new images based on the existing 26 captcha. New images would be written to <em>data/augmented</em> folder, while the labels are written to <em>data/output</em> folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "augmented_data = util.augment_captcha(preds_trocr_with_preprocessing_with_labels, captcha_solver.aug_dir, captcha_solver.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f6743",
   "metadata": {},
   "source": [
    "Now apply the TrOCR model on those new images in the <em>data/augmented</em> folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "captcha_solver = Captcha(device, \"../config.yaml\") # run again to reload the config.yaml\n",
    "\n",
    "preds_trocr_augmented_with_preprocessing = captcha_solver(mode = \"TrOCR\", im_path = \"../data/augmented\")  # let's use the augmented data only\n",
    "\n",
    "preds_trocr_augmented_with_preprocessing_with_labels = captcha_solver.add_gt_labels(preds_trocr_augmented_with_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f7844",
   "metadata": {},
   "source": [
    "### CER and WER calculation\n",
    "\n",
    "Time to evalaute the CER and WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = preds_trocr_augmented_with_preprocessing_with_labels['prediction'].tolist()\n",
    "references = preds_trocr_augmented_with_preprocessing_with_labels['ground_truth'].tolist()\n",
    "\n",
    "# Compute CER & WER\n",
    "cer_result = cer_metric.compute(predictions=predictions, references=references)\n",
    "wer_result = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Character Error Rate (CER): {cer_result:.2%}\")\n",
    "print(f\"Overall Word Error Rate (WER): {wer_result:.2%}\")\n",
    "\n",
    "# Detailed analysis per sample\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "# Iterate through DataFrame rows for detailed analysis\n",
    "for _, row in preds_trocr_augmented_with_preprocessing_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    key = row['key']\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    sample_cer = cer_metric.compute(predictions=[pred], references=[ref])\n",
    "    sample_wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "    \n",
    "    # Only show samples with errors\n",
    "    if sample_cer > 0 or sample_wer > 0:\n",
    "        print(f\"\\nSample {key}:\")\n",
    "        print(f\"Expected: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"CER: {sample_cer:.2%}\")\n",
    "        print(f\"WER: {sample_wer:.2%}\")\n",
    "        \n",
    "        # Show character-by-character comparison\n",
    "        print(\"Character differences:\")\n",
    "        for j, (c1, c2) in enumerate(zip(ref, pred)):\n",
    "            if c1 != c2:\n",
    "                print(f\"Position {j}: Expected '{c1}', Got '{c2}'\")\n",
    "        \n",
    "        # Handle different lengths\n",
    "        if len(ref) != len(pred):\n",
    "            print(f\"Length mismatch: Expected {len(ref)}, Got {len(pred)}\")\n",
    "            \n",
    "        # Show image file for reference\n",
    "        print(f\"Image file: {row['input_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2f105",
   "metadata": {},
   "source": [
    "On the augmented data, the performance on the 78 samples improved. CER drops to 7.95% from 3.85%, while WER drops to 25.64% from 15.38%. \n",
    "\n",
    "Let's do a character-level analysis to see which characters are recognized wronly more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7eaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store confusion matrix data\n",
    "confusion_data = {}\n",
    "\n",
    "# Iterate through all predictions\n",
    "for _, row in preds_trocr_augmented_with_preprocessing_with_labels.iterrows():\n",
    "    pred = row['prediction']\n",
    "    ref = row['ground_truth']\n",
    "    \n",
    "    # Compare characters\n",
    "    for expected, predicted in zip(ref, pred):\n",
    "        if expected != predicted:\n",
    "            # Initialize nested dictionary if needed\n",
    "            if expected not in confusion_data:\n",
    "                confusion_data[expected] = {}\n",
    "            if predicted not in confusion_data[expected]:\n",
    "                confusion_data[expected][predicted] = 0\n",
    "            \n",
    "            # Increment count\n",
    "            confusion_data[expected][predicted] += 1\n",
    "\n",
    "# Get all unique characters (both expected and predicted)\n",
    "all_chars = sorted(set(\n",
    "    list(confusion_data.keys()) + \n",
    "    [char for d in confusion_data.values() for char in d.keys()]\n",
    "))\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix_size = len(all_chars)\n",
    "confusion_matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "# Fill the confusion matrix\n",
    "for i, expected in enumerate(all_chars):\n",
    "    if expected in confusion_data:\n",
    "        for j, predicted in enumerate(all_chars):\n",
    "            if predicted in confusion_data[expected]:\n",
    "                confusion_matrix[i, j] = confusion_data[expected][predicted]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(confusion_matrix, \n",
    "            xticklabels=all_chars,\n",
    "            yticklabels=all_chars,\n",
    "            annot=True,  # Show numbers in cells\n",
    "            fmt='g',     # Format as integer\n",
    "            cmap='YlOrRd',  # Yellow to Orange to Red color scheme\n",
    "            square=True)    # Make cells square\n",
    "\n",
    "plt.title('Character Recognition Confusion Matrix')\n",
    "plt.xlabel('Predicted Character')\n",
    "plt.ylabel('Expected Character')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary of most common confusions:\")\n",
    "for expected in all_chars:\n",
    "    if expected in confusion_data and confusion_data[expected]:\n",
    "        print(f\"\\nCharacter '{expected}' was mistaken for:\")\n",
    "        sorted_mistakes = sorted(confusion_data[expected].items(), \n",
    "                               key=lambda x: x[1], \n",
    "                               reverse=True)\n",
    "        for predicted, count in sorted_mistakes:\n",
    "            print(f\"  - '{predicted}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f6a1",
   "metadata": {},
   "source": [
    "The heatmap above shows that digit \"0\" and alphabet \"O\" confused the model the most. This pair is known to be challenging for CV models, and visually difficult to diffentiate even for human eyes. Other difficult pairs here include \"S\" and \"5\", \"Z\" and \"2\". \n",
    "\n",
    "We could try to fine tune a base model with those augmented data. Due to time constraint, the fine-tuning was not run to complete. Below is the working code used for fine-tuning with torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2deba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# split data into train and test\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    augmented_data,\n",
    "    test_size=0.2,  # 20% for test set\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(augmented_data)}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Print sample distribution\n",
    "def print_distribution(df, name):\n",
    "    original = len(df[~df['augmented']])\n",
    "    augmented = len(df[df['augmented']])\n",
    "    print(f\"\\n{name} set distribution:\")\n",
    "    print(f\"Original images: {original}\")\n",
    "    print(f\"Augmented images: {augmented}\")\n",
    "    \n",
    "print_distribution(train_df, \"Training\")\n",
    "print_distribution(test_df, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f380c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_target_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame containing image paths and labels\n",
    "            processor: TrOCR processor for image and text processing\n",
    "            max_target_length: Maximum length for text encoding padding\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row from DataFrame\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(row['input_file']).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Encode the text label\n",
    "        labels = self.processor.tokenizer(\n",
    "            row['ground_truth'],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        \n",
    "        # Replace padding tokens with -100 for loss calculation\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        \n",
    "        # Return processed image and encoded labels\n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n",
    "        return encoding\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = CaptchaDataset(train_df, captcha_solver.processor)\n",
    "test_dataset = CaptchaDataset(test_df, captcha_solver.processor)\n",
    "\n",
    "# Verify the dataset\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Test a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nSample data:\")\n",
    "print(f\"Input shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Label shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test a single batch\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"\\nSample batch:\")\n",
    "print(f\"Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"Original text: {train_df['ground_truth'].iloc[0]}\")\n",
    "print(f\"Encoded labels: {sample['labels'].tolist()}\")\n",
    "labels = sample['labels']\n",
    "labels[labels == -100] = captcha_solver.processor.tokenizer.pad_token_id\n",
    "label_str = captcha_solver.processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create dataloader\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5611d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# prep for finetuning\n",
    "'''\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\") # microsoft/trocr-base-stage1 has not been fine-tuned on any dataset. \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80538ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# make some configurations \n",
    "'''\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = captcha_solver.processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = captcha_solver.processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = captcha_solver.processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# fine tuning\n",
    "'''\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):  \n",
    "   # train\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for batch in tqdm(train_dataloader):\n",
    "      # get the inputs\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "   \n",
    "\n",
    "model.save_pretrained(\"../model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captcha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
